import streamlit as st
import gspread
import pandas as pd
import numpy as np
import networkx as nx
import re
import difflib
import io
import zipfile
import time  # Added for sleep/backoff
from io import BytesIO
from math import radians
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import haversine_distances
from gspread.exceptions import APIError # Added for error handling

# --- CONFIGURATION ---
SHEET_NAME = "OverallMatchingInformation"
ADMIN_PASSWORD = "password"

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]

# --- CACHED RESOURCES ---
@st.cache_resource
def load_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

@st.cache_data
def load_city_database():
    url = "https://raw.githubusercontent.com/kelvins/US-Cities-Database/main/csv/us_cities.csv"
    try:
        ref_df = pd.read_csv(url)
        ref_df['MATCH_KEY'] = (ref_df['CITY'] + ", " + ref_df['STATE_CODE']).str.upper()
        ref_df = ref_df.drop_duplicates(subset=['MATCH_KEY'], keep='first')
        return {
            key: [lat, lon]
            for key, lat, lon in zip(ref_df['MATCH_KEY'], ref_df['LATITUDE'], ref_df['LONGITUDE'])
        }, list(ref_df['MATCH_KEY'])
    except Exception as e:
        st.error(f"Failed to load city database: {e}")
        return {}, []

# --- HELPERS WITH RETRY & CACHING ---

# 1. Cache the connection object so we don't re-authenticate constantly
@st.cache_resource
def get_gc():
    if "gcp_service_account" not in st.secrets:
        st.error("Missing 'gcp_service_account' in Streamlit secrets.")
        st.stop()
    creds_dict = dict(st.secrets["gcp_service_account"])
    return gspread.service_account_from_dict(creds_dict, scopes=SCOPES)

# 2. Retry wrapper to handle 429 errors automatically
def smart_read_sheet(sheet_object):
    """Tries to read a sheet, waits and retries if quota is hit."""
    for n in range(5): # Try 5 times
        try:
            return sheet_object.get_all_values()
        except APIError as e:
            if "429" in str(e):
                wait_time = (2 ** n) + 1 # Exponential backoff: 2s, 3s, 5s...
                time.sleep(wait_time)
            else:
                raise e
    return [] # Return empty if all retries fail

# 3. Cache the data fetching (TTL=600s means it refreshes every 10 mins automatically)
@st.cache_data(ttl=600)
def get_data(worksheet_name):
    """Gets all data and standardizes headers slightly."""
    try:
        gc = get_gc()
        # Open sheet but use retry logic for the actual read
        sheet = gc.open(SHEET_NAME).worksheet(worksheet_name)
        data = smart_read_sheet(sheet)
        
        if not data: return pd.DataFrame()
        df = pd.DataFrame(data[1:], columns=data[0])
        df.columns = df.columns.str.strip()
        return df
    except Exception as e:
        # Avoid error spam if sheet is just missing
        if worksheet_name != "PNM Information": 
            pass 
        return pd.DataFrame()

# 4. Cache the bulk loader used in "Run Matching"
@st.cache_data(ttl=600)
def load_google_sheet_data(sheet_name):
    """
    Loads data from Google Sheets using gspread and st.secrets.
    """
    try:
        gc = get_gc()
        sh = gc.open(sheet_name)

        # Helper to get DataFrame from worksheet with retry logic
        def get_df(ws_name):
            try:
                ws = sh.worksheet(ws_name)
                data = smart_read_sheet(ws)
                if not data:
                    return pd.DataFrame()
                df = pd.DataFrame(data[1:], columns=data[0])
                return df
            except gspread.WorksheetNotFound:
                st.error(f"Worksheet '{ws_name}' not found in the spreadsheet.")
                return None
            except Exception as e:
                return pd.DataFrame()

        # Load specific sheets
        bump_teams = get_df("Bump Teams")
        party_excuses = get_df("Party Excuses")
        pnm_info = get_df("PNM Information")
        mem_info = get_df("Member Information")
        prior_conn = get_df("Prior Connections")

        return bump_teams, party_excuses, pnm_info, mem_info, prior_conn

    except Exception as e:
        st.error(f"An error occurred connecting to Google Sheets: {e}")
        return None, None, None, None, None

def get_setting_value(cell):
    try:
        gc = get_gc()
        sheet = gc.open(SHEET_NAME).worksheet("Settings")
        return sheet.acell(cell).value
    except: return None

def update_settings(cell, value):
    try:
        gc = get_gc()
        gc.open(SHEET_NAME).worksheet("Settings").update_acell(cell, value)
        return True
    except: return False

def update_roster(names_list):
    try:
        gc = get_gc()
        ws = gc.open(SHEET_NAME).worksheet("Settings")
        ws.batch_clear(["D2:D1000"])
        names_list.sort()
        formatted = [[n] for n in names_list if n.strip()]
        if formatted: ws.update(range_name='D2', values=formatted)
        return True
    except: return False

def get_active_roster_names():
    # Attempt to pull from cache first via get_data if possible, 
    # but since this reads 'Settings' specifically (a custom range), we try/except it.
    try:
        gc = get_gc()
        sheet = gc.open(SHEET_NAME).worksheet("Settings")
        # Reading a single column is usually cheap, but could fail if quota is tight
        roster_data = sheet.get_values("D2:D")
        names = [r[0].strip() for r in roster_data if r and r[0].strip()]
        if names: return names
    except: pass

    # Fallback to member info (which IS cached now)
    df_mem = get_data("Member Information")
    if not df_mem.empty:
        possible_cols = ["Full Name", "Name", "Member Name", "Member"]
        found_col = None
        for col in df_mem.columns:
            if any(c.lower() in col.lower() for c in possible_cols):
                found_col = col
                break
        if found_col: return df_mem[found_col].dropna().astype(str).str.strip().tolist()
    return []

def update_team_ranking(team_id, new_ranking):
    try:
        gc = get_gc()
        sheet = gc.open(SHEET_NAME).worksheet("Bump Teams")
        cell = sheet.find(str(team_id), in_column=5)
        if cell:
            sheet.update_cell(cell.row, 6, new_ranking)
            # Clear cache so user sees update immediately
            get_data.clear()
            load_google_sheet_data.clear()
            return True
        return False
    except: return False

def batch_update_pnm_rankings(rankings_map):
    try:
        gc = get_gc()
        sheet = gc.open(SHEET_NAME).worksheet("PNM Information")
        all_values = smart_read_sheet(sheet) # Use smart read
        if not all_values: return 0
        headers = [h.lower().strip() for h in all_values[0]]
        
        try: id_idx = next(i for i, h in enumerate(headers) if 'pnm id' in h or 'id' == h)
        except: id_idx = 23
            
        try: rank_idx = next(i for i, h in enumerate(headers) if 'recruit rank' in h or 'average' in h)
        except: rank_idx = 24

        updates_count = 0
        for i in range(1, len(all_values)):
            row = all_values[i]
            if len(row) <= id_idx: continue
            p_id = str(row[id_idx]).strip()
            if p_id in rankings_map:
                while len(row) <= rank_idx: row.append("")
                row[rank_idx] = str(rankings_map[p_id])
                updates_count += 1
        sheet.update(values=all_values, range_name="A1")
        
        # Clear cache
        get_data.clear()
        load_google_sheet_data.clear()
        return updates_count
    except Exception as e:
        st.error(f"Batch update failed: {e}")
        return 0

def batch_update_team_rankings(rankings_map):
    try:
        gc = get_gc()
        sheet = gc.open(SHEET_NAME).worksheet("Bump Teams")
        all_values = smart_read_sheet(sheet) # Use smart read
        if not all_values: return 0
        headers = [h.lower().strip() for h in all_values[0]]
        
        try: id_idx = next(i for i, h in enumerate(headers) if 'team id' in h or 'id' == h)
        except: id_idx = 4
        try: rank_idx = next(i for i, h in enumerate(headers) if 'ranking' in h or 'rank' in h)
        except: rank_idx = 5

        updates_count = 0
        for i in range(1, len(all_values)):
            row = all_values[i]
            if len(row) <= id_idx: continue
            t_id = str(row[id_idx]).strip()
            if t_id in rankings_map:
                while len(row) <= rank_idx: row.append("")
                row[rank_idx] = str(rankings_map[t_id])
                updates_count += 1
        sheet.update(values=all_values, range_name="A1")
        
        # Clear cache
        get_data.clear()
        load_google_sheet_data.clear()
        return updates_count
    except Exception as e:
        st.error(f"Batch update failed: {e}")
        return 0

def auto_adjust_columns(writer, sheet_name, df):
    worksheet = writer.sheets[sheet_name]
    for idx, col in enumerate(df.columns):
        max_len = max(df[col].astype(str).map(len).max(), len(str(col))) + 2
        worksheet.set_column(idx, idx, max_len)

# --- MATCHING ALGORITHM HELPERS ---
def get_coords_offline(hometown_str, city_coords_map, all_city_keys):
    if not isinstance(hometown_str, str): return None, None
    key = hometown_str.strip().upper()
    if key in city_coords_map: return city_coords_map[key]
    matches = difflib.get_close_matches(key, all_city_keys, n=1, cutoff=0.8)
    if matches: return city_coords_map[matches[0]]
    return None, None

def extract_terms(row, cols):
    text_parts = [str(row.get(c, '')).lower() for c in cols]
    combined = ", ".join([p for p in text_parts if p != 'nan' and p.strip() != ''])
    return [t.strip() for t in combined.split(',') if t.strip()]

def get_year_tag(year_val):
    valid_years = ["Freshman", "Sophomore", "Junior", "Senior"]
    if pd.isna(year_val): return None
    raw = str(year_val).strip()
    matches = difflib.get_close_matches(raw, valid_years, n=1, cutoff=0.6)
    return matches[0] if matches else raw.title()

# --- MAIN PAGE ---
st.set_page_config(page_title="Admin Dashboard", layout="wide")
st.title("Sorority Admin Dashboard")

# Initialize Session State for Results
if "match_results" not in st.session_state:
    st.session_state.match_results = None

if "authenticated" not in st.session_state: st.session_state.authenticated = False

if not st.session_state.authenticated:
    pwd = st.text_input("Enter Admin Password:", type="password")
    if pwd == ADMIN_PASSWORD:
        st.session_state.authenticated = True
        st.rerun()
else:
    st.success("Logged in as Admin")

    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "Settings & Roster", "Member Information", "PNM Information and Rankings", 
        "View Bump Teams", "View Excuses", "View Prior Connections", "Run Matching"
    ])

    # --- TAB 1: SETTINGS ---
    with tab1:
        st.header("Event Configuration")
        current_parties = get_setting_value('B1')
        default_val = int(current_parties) if current_parties and str(current_parties).isdigit() else 4
        with st.form("party_config"):
            count = st.number_input("Number of Parties", 1, 50, default_val)
            if st.form_submit_button("Update Party Count"):
                if update_settings('B1', count): st.toast("Updated!")
        
        st.divider()
        st.header("Roster Management")
        col_r1, col_r2 = st.columns(2)
        with col_r1:
            st.subheader("Option A: Sync from Sheet")
            st.info("Pull names directly from the 'Member Information' tab.")
            # FORCE REFRESH HERE
            if st.button("ðŸ”„ Sync Roster from 'Member Information'"):
                st.cache_data.clear() # Clear cache to ensure we get fresh data
                df_source = get_data("Member Information")
                if not df_source.empty:
                    possible_cols = ["Full Name", "Name", "Member Name", "Member"]
                    found_col = None
                    for col in df_source.columns:
                        if any(c.lower() in col.lower() for c in possible_cols): found_col = col; break
                    if found_col:
                        names = df_source[found_col].astype(str).unique().tolist()
                        names = [n for n in names if n.strip()] 
                        if update_roster(names): st.success(f"âœ… Successfully synced {len(names)} members!")
                        else: st.error("Failed to update Settings.")
                    else: st.error("Could not find name column.")
                else: st.error("'Member Information' sheet is empty.")
        with col_r2:
            st.subheader("Option B: Upload CSV")
            st.info("Upload a CSV file to strictly override the roster names.")
            file = st.file_uploader("Upload Member List (CSV)", type="csv")
            if file:
                try:
                    df_upload = pd.read_csv(file)
                    new_names = []
                    name_col = next((c for c in df_upload.columns if "name" in c.lower()), None)
                    if name_col: new_names = df_upload[name_col].astype(str).tolist()
                    else: new_names = df_upload.iloc[:, 0].astype(str).tolist() if not df_upload.empty else []
                    new_names = [n for n in new_names if n.lower() != 'nan' and n.strip()]
                    if new_names:
                        st.success(f"Found {len(new_names)} names in CSV.")
                        if st.button("Override Roster with CSV"):
                            if update_roster(new_names): st.success("âœ… Roster overwritten!"); st.toast("Roster Overwritten!")
                            else: st.error("Failed to update settings.")
                    else: st.warning("No valid names found.")
                except Exception as e: st.error(f"Error reading CSV: {e}")

    # --- TAB 2: MEMBER INFORMATION ---
    with tab2:
        st.header("Member Information Database")
        # Updated to clear cache
        if st.button("ðŸ”„ Refresh Member Data"): 
            st.cache_data.clear()
            st.rerun()
            
        df_members = get_data("Member Information")
        if not df_members.empty:
            search_mem = st.text_input("ðŸ” Search Members:")
            if search_mem:
                mask = df_members.apply(lambda x: x.astype(str).str.contains(search_mem, case=False).any(), axis=1)
                display_df = df_members[mask]
            else: display_df = df_members
            st.metric("Total Members", len(display_df))
            st.dataframe(display_df, use_container_width=True)
        else: st.info("No member information found.")

    # --- TAB 3: PNM RANKINGS ---
    with tab3:
        st.header("PNM Ranking Management")
        df_votes = get_data("PNM Rankings")
        if not df_votes.empty:
            try:
                df_votes['Score'] = pd.to_numeric(df_votes['Score'], errors='coerce')
                id_col = next((c for c in df_votes.columns if 'pnm id' in c.lower()), None)
                if not id_col: id_col = next((c for c in df_votes.columns if 'id' in c.lower()), None)

                if id_col and 'Score' in df_votes.columns:
                    group_cols = [id_col]
                    name_col = next((c for c in df_votes.columns if 'pnm name' in c.lower()), None)
                    if name_col: group_cols.append(name_col)
                    
                    avg_df = df_votes.groupby(group_cols)['Score'].mean().reset_index()
                    avg_df.rename(columns={'Score': 'Calculated Average'}, inplace=True)
                    avg_df = avg_df.sort_values(by='Calculated Average', ascending=False)
                    
                    st.info(f"Processing {len(df_votes)} total votes across {len(avg_df)} unique PNMs...")
                    if st.button("Sync Rankings to PNM Sheet"):
                        with st.spinner("Syncing..."):
                            rankings_map = {str(row[id_col]).strip(): round(row['Calculated Average'], 2) for idx, row in avg_df.iterrows()}
                            count = batch_update_pnm_rankings(rankings_map)
                        st.success(f"âœ… Auto-synced {count} PNM rankings!")
                    
                    st.subheader("Raw Ranking Data")
                    st.dataframe(df_votes, use_container_width=True)
                else: st.error("Missing 'PNM ID' or 'Score' columns.")
            except Exception as e: st.error(f"Error processing rankings: {e}")
        else: st.info("No votes found in 'PNM Rankings' sheet yet.")
        st.divider()
        st.subheader("Current PNM Database")
        df_pnms = get_data("PNM Information")
        if not df_pnms.empty:
            pnm_search = st.text_input("ðŸ” Search PNM Database:")
            display_pnm = df_pnms[df_pnms.astype(str).apply(lambda x: x.str.contains(pnm_search, case=False).any(), axis=1)] if pnm_search else df_pnms
            st.dataframe(display_pnm, use_container_width=True)
        else: st.info("No PNM data found.")

    # --- TAB 4: VIEW BUMP TEAMS ---
    with tab4:
        st.header("Bump Team Management")
        df_teams = get_data("Bump Teams")
        if not df_teams.empty:
            id_col = next((c for c in df_teams.columns if 'team id' in c.lower() or 'id' in c.lower()), df_teams.columns[4] if len(df_teams.columns)>4 else None)
            creator_col = next((c for c in df_teams.columns if 'creator' in c.lower()), df_teams.columns[1] if len(df_teams.columns)>1 else None)
            partners_col = next((c for c in df_teams.columns if 'partner' in c.lower()), df_teams.columns[2] if len(df_teams.columns)>2 else None)
            rank_col = next((c for c in df_teams.columns if 'rank' in c.lower()), None)

            if id_col and creator_col:
                df_teams['display_label'] = df_teams.apply(lambda x: f"Team {x[id_col]} | {x[creator_col]}, {x.get(partners_col, '')}", axis=1)
                t1, t2 = st.tabs(["Single Team Recruiter Ranking Update", "Bulk Team Recruiter Ranking Upload (CSV)"])
                
                with t1:
                    c1, c2, c3 = st.columns([3, 1, 1])
                    with c1:
                        sel_label = st.selectbox("Select Team to Rank:", df_teams['display_label'].tolist())
                        sel_id = df_teams[df_teams['display_label'] == sel_label][id_col].values[0]
                    with c2:
                        cur_rank = df_teams[df_teams[id_col] == sel_id][rank_col].values[0] if rank_col else 1
                        try: init_val = int(cur_rank)
                        except: init_val = 1
                        new_rank = st.number_input(f"Assign Rank:", min_value=1, value=init_val, key="team_rank_input")
                    with c3:
                        st.markdown("<br>", unsafe_allow_html=True)
                        if st.button("Save Team Rank"):
                            if update_team_ranking(sel_id, new_rank): st.success(f"Rank {new_rank} assigned!"); st.rerun()

                with t2:
                    st.info('Upload a CSV with columns: "Team ID" (or "Creator Name") and "Ranking".')
                    team_csv = st.file_uploader("Upload Rankings CSV", type=["csv"], key="team_rank_upload")
                    if team_csv and st.button("Process Bulk Update"):
                        try:
                            df_b = pd.read_csv(team_csv)
                            df_b.columns = df_b.columns.str.strip().str.lower()
                            b_id = next((c for c in df_b.columns if 'id' in c), None)
                            b_name = next((c for c in df_b.columns if 'name' in c or 'creator' in c), None)
                            b_rank = next((c for c in df_b.columns if 'rank' in c), None)
                            
                            if b_rank and (b_id or b_name):
                                bulk_map = {}
                                name_map = dict(zip(df_teams[creator_col].astype(str).str.strip().str.lower(), df_teams[id_col].astype(str))) if b_name else {}
                                for _, r in df_b.iterrows():
                                    rv = r[b_rank]
                                    tid = None
                                    if b_id and pd.notna(r[b_id]): tid = str(int(r[b_id])) if str(r[b_id]).replace('.','').isdigit() else str(r[b_id])
                                    elif b_name and pd.notna(r[b_name]): tid = name_map.get(str(r[b_name]).strip().lower())
                                    if tid: bulk_map[tid] = rv
                                if bulk_map:
                                    cnt = batch_update_team_rankings(bulk_map)
                                    st.success(f"âœ… Updated {cnt} teams!"); st.rerun()
                                else: st.warning("No valid teams found.")
                            else: st.error("CSV missing required columns.")
                        except Exception as e: st.error(f"Error: {e}")
            st.divider()
            st.dataframe(df_teams.drop(columns=['display_label'], errors='ignore'), use_container_width=True)
        else: st.info("No bump teams found yet.")

    # --- TAB 5 & 6: EXCUSES & CONNECTIONS ---
    with tab5:
        st.header("Member Party Excuses")
        if st.button("ðŸ”„ Refresh Excuses"): 
            st.cache_data.clear()
            st.rerun()
        df_ex = get_data("Party Excuses")
        if not df_ex.empty: st.dataframe(df_ex, use_container_width=True)
        else: st.info("No excuses found.")
    with tab6:
        st.header("Prior PNM Connections Log")
        if st.button("ðŸ”„ Refresh Connections"): 
            st.cache_data.clear()
            st.rerun()
        df_conn = get_data("Prior Connections")
        if not df_conn.empty: st.dataframe(df_conn, use_container_width=True)
        else: st.info("No prior connections found.")

    # --- TAB 7: RUN MATCHING ---
    with tab7:
        st.header("Run Matching Algorithm")
        
        st.subheader("Matching Algorithm Settings")
        try:
            setting_parties = get_setting_value('B1')
            num_parties = int(setting_parties) if setting_parties and str(setting_parties).isdigit() else 4
        except:
            num_parties = 4
            
        st.info(f"**Total Parties:** {num_parties} (Synced from Settings & Roster Tab)")
        pnms_per_party = st.number_input("PNMs Per Party", min_value=1, value=45)
        matches_per_team = st.number_input("Matches per Bump Team (Capacity)", min_value=1, value=2)
        num_rounds = st.number_input("Rounds per Party", min_value=1, value=4)
        bump_order_set = st.radio("Is Bump Order Set?", ("Yes", "No"), horizontal=True)
        is_bump_order_set = "y" if bump_order_set == "Yes" else "n"

        st.divider()
        run_button = st.button("Run Matching Algorithm", type="primary", use_container_width=True)

        # --- MAIN LOGIC ---
        if run_button:
            with st.spinner("Connecting to Google Sheets & Loading Data..."):
                # Load Data from Google Sheets (Hardcoded SHEET_NAME)
                # This uses the CACHED function now
                bump_teams, party_excuses, pnm_intial_interest, member_interest, member_pnm_no_match = load_google_sheet_data(SHEET_NAME)

            if any(df is None for df in [bump_teams, party_excuses, pnm_intial_interest, member_interest, member_pnm_no_match]):
                st.stop() # Error messages handled in load function
            
            # --- VALIDATION CHECK: CAPACITY ---
            total_pnms_in_dataset = len(pnm_intial_interest)
            total_slots = num_parties * pnms_per_party
            
            if total_pnms_in_dataset > total_slots:
                st.error(
                    f"âŒ **Not enough spots!**\n\n"
                    f"You have **{total_pnms_in_dataset}** PNMs in your database, but only **{total_slots}** available spots "
                    f"({num_parties} parties Ã— {pnms_per_party} PNMs/party).\n\n"
                    f"Please increase the number of parties or the number of PNMs per party to accommodate everyone."
                )
                st.stop()
            # ----------------------------------

            with st.spinner("Initializing Models & Processing Data..."):
                # Clean Columns
                for df in [bump_teams, party_excuses, pnm_intial_interest, member_interest, member_pnm_no_match]:
                    df.columns = df.columns.str.strip()
                
                # Load NLP & Geo Resources
                model = load_model()
                city_coords_map, all_city_keys = load_city_database()

                # --- STEP 1: PARTY ASSIGNMENT & CLUSTERING ---
                with st.status("Preprocessing & Clustering...", expanded=True) as status:
                    st.write("Assigning Parties...")
                    # REMOVED SLICING: Now using full dataframe
                    # pnm_intial_interest = pnm_intial_interest.iloc[0:pnms_to_process].copy() 
                    pnm_intial_interest = pnm_intial_interest.copy()

                    party_assignments = np.tile(np.arange(1, num_parties + 1), int(pnms_per_party))
                    
                    if len(party_assignments) != len(pnm_intial_interest):
                        diff = len(pnm_intial_interest) - len(party_assignments)
                        if diff > 0: party_assignments = np.concatenate([party_assignments, np.arange(1, diff+1)])
                        else: party_assignments = party_assignments[:len(pnm_intial_interest)]

                    np.random.seed(42)
                    np.random.shuffle(party_assignments)
                    pnm_intial_interest['Party'] = party_assignments

                    st.write("Geocoding & Analyzing Interests...")
                    # Standardize PNM Columns
                    pnm_col_map = {
                        'PNM Name': 'Full Name', 'Enter your hometown in the form City, State:': 'Hometown',
                        'Enter your major or "Undecided":': 'Major', 'Enter your minor or leave blank:': 'Minor',
                        'Enter your high school involvement (sports, clubs etc.), separate each activity by a comma:': 'High School Involvement',
                        'Enter your college involvement (sports, clubs etc.), separate each activity by a comma:': 'College Involvement',
                        'Enter your hobbies and interests, separate each activity by a comma:': 'Hobbies',
                        'Pick your year in school:': 'Year'
                    }
                    pnm_clean = pnm_intial_interest.rename(columns=pnm_col_map)
                    df_mem = member_interest.copy()
                    
                    # --- CLUSTERING LOGIC ---
                    all_coords, geo_tracker = [], []
                    for idx, row in df_mem.iterrows():
                        lat, lon = get_coords_offline(row.get('Hometown'), city_coords_map, all_city_keys)
                        if lat: 
                            all_coords.append([radians(lat), radians(lon)])
                            geo_tracker.append({'type': 'mem', 'id': row['Sorority ID'], 'hometown': row['Hometown']})
                    for idx, row in pnm_clean.iterrows():
                        lat, lon = get_coords_offline(row.get('Hometown'), city_coords_map, all_city_keys)
                        if lat:
                            all_coords.append([radians(lat), radians(lon)])
                            geo_tracker.append({'type': 'pnm', 'id': row['PNM ID'], 'hometown': row['Hometown']})

                    mem_geo_tags, pnm_geo_tags = {}, {}
                    if all_coords:
                        dist_matrix = haversine_distances(all_coords, all_coords) * 3958.8
                        geo_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=30, metric='precomputed', linkage='single')
                        geo_labels = geo_clustering.fit_predict(dist_matrix)
                        geo_groups = {}
                        for i, label in enumerate(geo_labels):
                            if label not in geo_groups: geo_groups[label] = []
                            geo_groups[label].append(geo_tracker[i]['hometown'])
                        for i, label in enumerate(geo_labels):
                            group_name = geo_groups[label][0]
                            tracker = geo_tracker[i]
                            if tracker['type'] == 'mem': mem_geo_tags[tracker['id']] = group_name
                            else: pnm_geo_tags[tracker['id']] = group_name

                    all_terms_list, mem_interest_map, pnm_interest_map = [], [], []
                    cols_to_extract = ['Major', 'Minor', 'Hobbies', 'College Involvement', 'High School Involvement']
                    for idx, row in df_mem.iterrows():
                        terms = extract_terms(row, cols_to_extract)
                        for term in terms: all_terms_list.append(term); mem_interest_map.append({'id': row['Sorority ID'], 'term': term})
                    for idx, row in pnm_clean.iterrows():
                        terms = extract_terms(row, cols_to_extract)
                        for term in terms: all_terms_list.append(term); pnm_interest_map.append({'id': row['PNM ID'], 'term': term})

                    term_to_group = {}
                    if all_terms_list:
                        unique_terms = list(set(all_terms_list))
                        embeddings = model.encode(unique_terms)
                        sem_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5, metric='cosine', linkage='average')
                        sem_labels = sem_clustering.fit_predict(embeddings)
                        temp_map = {}
                        for term, label in zip(unique_terms, sem_labels):
                            if label not in temp_map: temp_map[label] = []
                            temp_map[label].append(term)
                        for label, terms in temp_map.items():
                            attr_name = min(terms, key=len)
                            for term in terms: term_to_group[term] = attr_name

                    def finalize_attributes(df, id_col, geo_tags, int_map):
                        final_attrs = {row[id_col]: set() for _, row in df.iterrows()}
                        for idx, row in df.iterrows():
                            pid = row[id_col]
                            yt = get_year_tag(row.get('Year'))
                            if yt: final_attrs[pid].add(yt)
                            if pid in geo_tags: final_attrs[pid].add(geo_tags[pid])
                        for entry in int_map:
                            pid = entry['id']
                            if entry['term'] in term_to_group: final_attrs[pid].add(term_to_group[entry['term']])
                        return df[id_col].map(lambda x: ", ".join(final_attrs.get(x, set())))

                    member_interest['attributes_for_matching'] = finalize_attributes(df_mem, 'Sorority ID', mem_geo_tags, mem_interest_map)
                    pnm_intial_interest['attributes_for_matching'] = finalize_attributes(pnm_clean, 'PNM ID', pnm_geo_tags, pnm_interest_map)
                    
                    status.update(label="Preprocessing Complete!", state="complete", expanded=False)

                # --- STEP 2: CALCULATE GLOBAL RANKING STATS ---
                # This ensures the ranking bonus is agnostic to the scale (1-3, 1-5, etc.)
                try:
                    all_ranks = pd.to_numeric(pnm_intial_interest['Average Recruit Rank'], errors='coerce')
                    min_obs = all_ranks.min()
                    if pd.isna(min_obs): min_obs = 1.0
                    all_ranks = all_ranks.fillna(min_obs)
                    global_max = all_ranks.max()
                    global_min = all_ranks.min()
                    
                    # Avoid division by zero if everyone has the same rank
                    if global_max == global_min: global_max += 1.0 
                except Exception as e:
                    st.error(f"Error calculating global ranking stats: {e}")
                    global_max, global_min = 5.0, 1.0 # Fallback

                # --- STEP 3: CORE MATCHING LOGIC ---
                progress_bar = st.progress(0)
                status_text = st.empty()
                zip_buffer = BytesIO()

                # Pre-process Data for Loop
                party_excuses["Choose the party/parties you are unable to attend:"] = party_excuses["Choose the party/parties you are unable to attend:"].apply(
                    lambda x: [int(i) for i in re.findall(r'\d+', str(x))] if pd.notnull(x) else []
                )
                party_excuses = party_excuses.explode("Choose the party/parties you are unable to attend:")

                member_pnm_no_match["PNM Name"] = member_pnm_no_match["PNM Name"].str.split(r',\s*', regex=True)
                member_pnm_no_match = member_pnm_no_match.explode("PNM Name")

                no_match_pairs = {
                    (row["Member Name"], row["PNM Name"])
                    for row in member_pnm_no_match.to_dict('records')
                }

                member_attr_cache = {
                    row['Sorority ID']: set(str(row.get('attributes_for_matching', '')).split(', '))
                    if row.get('attributes_for_matching') else set()
                    for row in member_interest.to_dict('records')
                }
                name_to_id_map = member_interest.set_index('Full Name')['Sorority ID'].to_dict()

                all_member_traits = member_interest['attributes_for_matching'].str.split(', ').explode()
                trait_freq = all_member_traits.value_counts()
                trait_weights = (len(member_interest) / trait_freq).to_dict()
                strength_bonus_map = {1: 1.5, 2: 1.0, 3: 0.5, 4: 0.0}

                # Conversion helper for strings from GSheet
                def to_float(val, default=1.0):
                    try: return float(val)
                    except: return default
                
                def to_int(val, default=4):
                    try: return int(val)
                    except: return default

                # === FIX START: Added compression and ensure download is OUTSIDE the 'with' block ===
                
                # List to store individual file data for later download buttons
                individual_party_files = []
                
                with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                    for party in range(1, int(num_parties) + 1):
                        progress_bar.progress(party / num_parties)
                        status_text.text(f"Processing Party {party}...")

                        pnms_df = pnm_intial_interest[pnm_intial_interest['Party'] == party].copy()
                        if pnms_df.empty: continue

                        pnm_list = []
                        pnm_records = pnms_df.to_dict('records')

                        for i, row in enumerate(pnm_records):
                            p_attrs = set(str(row['attributes_for_matching']).split(', '))
                            p_rank_val = to_float(row.get("Average Recruit Rank", 1.0))
                            
                            # --- AGNOSTIC BONUS CALCULATION ---
                            # Clamp val to observed range just in case
                            safe_rank = max(global_min, min(p_rank_val, global_max))
                            
                            # Calculate relative strength (0.0 to 1.0)
                            relative_strength = (safe_rank - global_min) / (global_max - global_min)
                            
                            # Weight: 3.0 means a perfect recruit gets +3.0 score benefit
                            RANKING_WEIGHT = 3.0 
                            pnm_bonus = relative_strength * RANKING_WEIGHT
                            # ----------------------------------

                            pnm_list.append({
                                'idx': i, 
                                'id': row['PNM ID'], 
                                'name': row.get('PNM Name', row.get('Full Name')),
                                'attrs': p_attrs, 
                                'rank': p_rank_val, 
                                'bonus': pnm_bonus, # Updated bonus
                                'node_id': f"p_{i}"
                            })

                        party_excused_names = set(party_excuses[party_excuses["Choose the party/parties you are unable to attend:"] == party]["Member Name"])

                        team_list = []
                        broken_teams_list = []

                        for raw_idx, row in enumerate(bump_teams.to_dict('records')):
                            submitter = row["Creator Name"]
                            partners_str = str(row.get("Bump Partners", ""))
                            if partners_str.lower() == 'nan': partners = []
                            else: partners = [p.strip() for p in re.split(r'[,;]\s*', partners_str) if p.strip()]

                            current_members = [submitter] + partners
                            missing_members = [m for m in current_members if m in party_excused_names]

                            if missing_members:
                                broken_teams_list.append({'members': current_members, 'missing': missing_members})
                            else:
                                t_rank = to_int(row.get("Ranking", 4))
                                team_list.append({
                                    't_idx': len(team_list), 'members': current_members, 'team_size': len(current_members),
                                    'member_ids': [name_to_id_map.get(m) for m in current_members],
                                    'joined_names': ", ".join(current_members), 'bonus': strength_bonus_map.get(t_rank, 0.0),
                                    'node_id': f"t_{len(team_list)}", 'row_data': row
                                })

                        # --- Capacity Checks ---
                        total_capacity = len(team_list) * matches_per_team
                        
                        potential_pairs = []
                        for p_data in pnm_list:
                            for t_data in team_list:
                                if any((m, p_data['name']) in no_match_pairs for m in t_data['members']): continue
                                
                                score = 0
                                reasons_list = []
                                for m_id, m_name in zip(t_data['member_ids'], t_data['members']):
                                    if m_id is None: continue
                                    m_attrs = member_attr_cache.get(m_id, set())
                                    shared = p_data['attrs'].intersection(m_attrs)
                                    if shared:
                                        score += sum(trait_weights.get(t, 1.0) for t in shared)
                                        reasons_list.append(f"{p_data['name']} has {', '.join(shared)} with {m_name}.")
                                
                                total_score = score + t_data['bonus'] + p_data['bonus']
                                final_cost = 1 / (1 + total_score)
                                potential_pairs.append({
                                    'p_id': p_data['id'], 'p_name': p_data['name'], 'p_attrs': p_data['attrs'],
                                    't_idx': t_data['t_idx'], 'team_size': t_data['team_size'],
                                    'p_node': p_data['node_id'], 't_node': t_data['node_id'],
                                    'cost': final_cost, 'pnm_rank': p_data['rank'],
                                    'team_members': t_data['joined_names'],
                                    'reasons': " ".join(reasons_list) if reasons_list else "No specific match"
                                })

                        potential_pairs.sort(key=lambda x: (x['cost'], -x['pnm_rank']))
                        matchable_pnm_ids = {p['p_id'] for p in potential_pairs}

                        # --- PHASE A: GLOBAL MATCHING ---
                        global_flow_results = []
                        assignments_map_flow = {t['t_idx']: [] for t in team_list}

                        G = nx.DiGraph()
                        source, sink, no_match_node = 'source', 'sink', 'dummy_nomatch'
                        total_flow = len(pnm_list)

                        G.add_node(source, demand=-total_flow)
                        G.add_node(sink, demand=total_flow)
                        G.add_node(no_match_node)

                        for p in pnm_list:
                            G.add_edge(source, p['node_id'], capacity=1, weight=0)
                            G.add_edge(p['node_id'], no_match_node, capacity=1, weight=1000000)

                        for t in team_list:
                            G.add_edge(t['node_id'], sink, capacity=matches_per_team, weight=0)

                        G.add_edge(no_match_node, sink, capacity=total_flow, weight=0)

                        for pair in potential_pairs:
                            G.add_edge(pair['p_node'], pair['t_node'], capacity=1, weight=int(pair['cost'] * 10000))

                        try:
                            flow_dict = nx.min_cost_flow(G)
                            pair_lookup = {(p['p_node'], p['t_node']): p for p in potential_pairs}
                            pnm_ids_with_potential = {p['p_id'] for p in potential_pairs}

                            for p_data in pnm_list:
                                p_node = p_data['node_id']
                                if p_node in flow_dict:
                                    for t_node, flow in flow_dict[p_node].items():
                                        if flow > 0:
                                            if t_node == no_match_node:
                                                reason = "Conflict List" if p_data['id'] not in pnm_ids_with_potential else "Capacity Reached"
                                                global_flow_results.append({
                                                    'PNM ID': p_data['id'], 'PNM Name': p_data['name'],
                                                    'Bump Team Members': "NO MATCH", 'Match Cost': None, 'Reason': reason
                                                })
                                            else:
                                                match_info = pair_lookup.get((p_node, t_node))
                                                if match_info:
                                                    global_flow_results.append({
                                                        'PNM ID': p_data['id'], 'PNM Name': p_data['name'],
                                                        'Bump Team Members': match_info['team_members'], 'Match Cost': round(match_info['cost'], 4),
                                                        'Reason': match_info['reasons']
                                                    })
                                                    assignments_map_flow[match_info['t_idx']].append(match_info)
                        except nx.NetworkXUnfeasible:
                            st.warning(f"Global Flow Unfeasible for Party {party}")

                        # --- A2: GLOBAL GREEDY ---
                        global_greedy_results = []
                        assignments_map_greedy = {t['t_idx']: [] for t in team_list}
                        matched_pnm_ids = set()
                        team_counts = {t['t_idx']: 0 for t in team_list}

                        for pair in potential_pairs:
                            if pair['p_id'] not in matched_pnm_ids:
                                if team_counts[pair['t_idx']] < matches_per_team:
                                    matched_pnm_ids.add(pair['p_id'])
                                    team_counts[pair['t_idx']] += 1
                                    global_greedy_results.append({
                                        'PNM ID': pair['p_id'], 'PNM Name': pair['p_name'],
                                        'Bump Team Members': pair['team_members'], 'Match Cost': round(pair['cost'], 4),
                                        'Reason': pair['reasons']
                                    })
                                    assignments_map_greedy[pair['t_idx']].append(pair)

                        for p_data in pnm_list:
                            if p_data['id'] not in matched_pnm_ids:
                                was_blocked = not any(p['p_id'] == p_data['id'] for p in potential_pairs)
                                reason = "Conflict List" if was_blocked else "Capacity Reached (Greedy)"
                                global_greedy_results.append({
                                    'PNM ID': p_data['id'], 'PNM Name': p_data['name'],
                                    'Bump Team Members': "NO MATCH", 'Match Cost': None, 'Reason': reason
                                })

                        # --- PHASE B: INTERNAL ROTATIONS ---
                        def run_internal_rotation(assignment_map, method='flow'):
                            rotation_output = []
                            actual_rounds = 1 if is_bump_order_set == 'y' else num_rounds

                            for t_idx, assigned_pnms in assignment_map.items():
                                if not assigned_pnms: continue
                                team_data = next((t for t in team_list if t['t_idx'] == t_idx), None)
                                if not team_data: continue

                                raw_rgl = team_data['row_data'].get('RGL', '')
                                team_rgl_name = "" if pd.isna(raw_rgl) or str(raw_rgl).lower() == 'nan' else str(raw_rgl).strip()
                                
                                valid_members = []
                                for m_id, m_name in zip(team_data['member_ids'], team_data['members']):
                                    if m_id: valid_members.append({'id': m_id, 'name': m_name})

                                history = set()
                                for round_num in range(1, actual_rounds + 1):
                                    if round_num == 1 and team_rgl_name:
                                        active_members = [m for m in valid_members if m['name'].strip() != team_rgl_name]
                                    else:
                                        active_members = valid_members

                                    must_allow_repeats = round_num > len(active_members)

                                    if method == 'flow':
                                        sub_G = nx.DiGraph()
                                        sub_s, sub_t = 's', 't'
                                        req = len(assigned_pnms)
                                        sub_G.add_node(sub_s, demand=-req)
                                        sub_G.add_node(sub_t, demand=req)

                                        for p in assigned_pnms: sub_G.add_edge(sub_s, f"p_{p['p_id']}", capacity=1, weight=0)
                                        for m in active_members: sub_G.add_edge(f"m_{m['id']}", sub_t, capacity=1, weight=0)

                                        for p in assigned_pnms:
                                            for m in active_members:
                                                # Ensure string comparison for history check
                                                is_repeat = (str(p['p_id']), str(m['id'])) in history
                                                if is_repeat and not must_allow_repeats: continue
                                                m_attrs = member_attr_cache.get(m['id'], set())
                                                shared = p['p_attrs'].intersection(m_attrs)
                                                score = sum(trait_weights.get(t, 1.0) for t in shared)
                                                base_cost = int((1/(1+score))*10000)
                                                final_cost = base_cost + 50000 if is_repeat else base_cost
                                                reason = ", ".join(shared) if shared else "Rotation"
                                                if is_repeat: reason += " (Repeat)"
                                                sub_G.add_edge(f"p_{p['p_id']}", f"m_{m['id']}", capacity=1, weight=final_cost, reason=reason)
                                            
                                        try:
                                            sub_flow = nx.min_cost_flow(sub_G)
                                            for p in assigned_pnms:
                                                p_node = f"p_{p['p_id']}"
                                                if p_node in sub_flow:
                                                    for tgt, flow in sub_flow[p_node].items():
                                                        if flow > 0 and tgt != sub_t:
                                                            # Fix: Keep ID as string to match Google Sheets data format
                                                            raw_id = tgt.replace("m_", "")
                                                            m_name = next((m['name'] for m in valid_members if str(m['id']) == raw_id), "Unknown")
                                                            
                                                            edge_d = sub_G.get_edge_data(p_node, tgt)
                                                            calc_cost = (edge_d.get('weight', 10000) - (50000 if edge_d.get('weight',0) > 40000 else 0)) / 10000.0
                                                            
                                                            rotation_output.append({
                                                                'Round': round_num, 'Team ID': t_idx, 'Team Members': team_data['joined_names'],
                                                                'PNM ID': p['p_id'], 'PNM Name': p['p_name'], 'Matched Member': m_name,
                                                                'Match Cost': round(calc_cost, 4), 'Reason': f"Common: {edge_d.get('reason')}"
                                                            })
                                                            # Add to history as strings
                                                            history.add((str(p['p_id']), str(raw_id)))
                                        except nx.NetworkXUnfeasible:
                                            rotation_output.append({'Round': round_num, 'Team ID': t_idx, 'PNM Name': "FLOW FAIL", 'Reason': "Unfeasible"})
                                    
                                    elif method == 'greedy':
                                        candidates = []
                                        for p in assigned_pnms:
                                            for m in active_members:
                                                is_repeat = (str(p['p_id']), str(m['id'])) in history
                                                if is_repeat and not must_allow_repeats: continue
                                                m_attrs = member_attr_cache.get(m['id'], set())
                                                shared = p['p_attrs'].intersection(m_attrs)
                                                score = sum(trait_weights.get(t, 1.0) for t in shared)
                                                final_score = score - 1000 if is_repeat else score
                                                reason = ", ".join(shared) if shared else "Rotation"
                                                if is_repeat: reason += " (Repeat)"
                                                candidates.append((final_score, p, m, reason, is_repeat))
                                            
                                        candidates.sort(key=lambda x: x[0], reverse=True)
                                        
                                        round_pnm_done, round_mem_done = set(), set()
                                        for sc, p, m, rs, is_rep in candidates:
                                            if p['p_id'] not in round_pnm_done and m['id'] not in round_mem_done:
                                                real_score = sc + 1000 if is_rep else sc
                                                rotation_output.append({
                                                    'Round': round_num, 'Team ID': t_idx, 'Team Members': team_data['joined_names'],
                                                    'PNM ID': p['p_id'], 'PNM Name': p['p_name'], 'Matched Member': m['name'],
                                                    'Match Cost': round(1.0/(1.0+real_score), 4), 'Reason': f"Common: {rs}" if real_score > 0 else "Greedy Fill"
                                                })
                                                round_pnm_done.add(p['p_id']); round_mem_done.add(m['id'])
                                                history.add((str(p['p_id']), str(m['id'])))
                            return rotation_output

                        internal_flow_results = run_internal_rotation(assignments_map_flow, method='flow')
                        internal_greedy_results = run_internal_rotation(assignments_map_greedy, method='greedy')

                        # --- PHASE C: BUMP INSTRUCTIONS ---
                        def generate_bump_instructions(rotation_data):
                            if not rotation_data: return []
                            df = pd.DataFrame(rotation_data)
                            if df.empty or 'Matched Member' not in df.columns: return []
                            df = df.sort_values(by=['Team ID', 'PNM ID', 'Round'])
                            df['Person_To_Bump'] = df.groupby(['Team ID', 'PNM ID'])['Matched Member'].shift(1)
                            instructions = df[df['Person_To_Bump'].notna()].copy()
                            instructions['At End Of Round'] = instructions['Round'] - 1
                            output = instructions[['Matched Member', 'At End Of Round', 'Person_To_Bump', 'PNM Name']].rename(columns={
                                'Matched Member': 'Member (You)', 'Person_To_Bump': 'Go Bump This Person', 'PNM Name': 'Who is with PNM'
                            })
                            return output.sort_values(by=['Member (You)', 'At End Of Round']).to_dict('records')

                        bump_instruct_flow = generate_bump_instructions(internal_flow_results)
                        bump_instruct_greedy = generate_bump_instructions(internal_greedy_results)

                        # --- EXPORT TO EXCEL ---
                        if global_flow_results:
                            output = BytesIO()
                            df_glob_flow = pd.DataFrame(global_flow_results)
                            df_glob_greedy = pd.DataFrame(global_greedy_results)
                            df_rot_flow = pd.DataFrame(internal_flow_results)
                            df_rot_greedy = pd.DataFrame(internal_greedy_results)
                            df_bump_flow = pd.DataFrame(bump_instruct_flow)
                            df_bump_greedy = pd.DataFrame(bump_instruct_greedy)

                            # --- SUMMARY CALCULATION ---
                            flow_costs = df_glob_flow['Match Cost'].dropna()
                            greedy_costs = df_glob_greedy['Match Cost'].dropna()

                            summary_data = {
                                'Metric': ['Total Cost', 'Average Cost', 'Min Cost', 'Max Cost', 'Std Dev'],
                                'Global Network Flow': [
                                    round(flow_costs.sum(), 4),
                                    round(flow_costs.mean(), 4) if not flow_costs.empty else 0,
                                    round(flow_costs.min(), 4) if not flow_costs.empty else 0,
                                    round(flow_costs.max(), 4) if not flow_costs.empty else 0,
                                    round(flow_costs.std(), 4) if len(flow_costs) > 1 else 0
                                ],
                                'Global Greedy': [
                                    round(greedy_costs.sum(), 4),
                                    round(greedy_costs.mean(), 4) if not greedy_costs.empty else 0,
                                    round(greedy_costs.min(), 4) if not greedy_costs.empty else 0,
                                    round(greedy_costs.max(), 4) if not greedy_costs.empty else 0,
                                    round(greedy_costs.std(), 4) if len(greedy_costs) > 1 else 0
                                ]
                            }
                            summary_df = pd.DataFrame(summary_data)

                            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                                summary_df.to_excel(writer, sheet_name="Summary_Comparison", index=False); auto_adjust_columns(writer, "Summary_Comparison", summary_df)
                                df_glob_flow.to_excel(writer, sheet_name="Global_Matches_Flow", index=False); auto_adjust_columns(writer, "Global_Matches_Flow", df_glob_flow)
                                df_glob_greedy.to_excel(writer, sheet_name="Global_Matches_Greedy", index=False); auto_adjust_columns(writer, "Global_Matches_Greedy", df_glob_greedy)
                                
                                if not df_rot_flow.empty:
                                    if is_bump_order_set == "n":
                                        # MODIFIED: Drop 'Team ID' and 'Team Members' for Rotation Flow export
                                        rot_flow_out = df_rot_flow.drop(columns=['Team ID', 'Team Members'], errors='ignore')
                                        rot_flow_out.to_excel(writer, sheet_name="Rotation_Flow", index=False)
                                        auto_adjust_columns(writer, "Rotation_Flow", rot_flow_out)
                                        
                                        if not df_bump_flow.empty: df_bump_flow.to_excel(writer, sheet_name="Bump_Logistics_Flow", index=False); auto_adjust_columns(writer, "Bump_Logistics_Flow", df_bump_flow)
                                    else:
                                        # MODIFIED: Drop 'Team ID', 'Round', and 'Team Members' for Round 1 Matches
                                        r1 = df_rot_flow[df_rot_flow['Round'] == 1].drop(columns=['Team ID', 'Round', 'Team Members'], errors='ignore')
                                        r1.to_excel(writer, sheet_name="Round_1_Matches_Flow", index=False)
                                        auto_adjust_columns(writer, "Round_1_Matches_Flow", r1)
                                
                                if not df_rot_greedy.empty:
                                    if is_bump_order_set == "n":
                                        # MODIFIED: Drop 'Team ID' and 'Team Members' for Rotation Greedy export
                                        rot_greedy_out = df_rot_greedy.drop(columns=['Team ID', 'Team Members'], errors='ignore')
                                        rot_greedy_out.to_excel(writer, sheet_name="Rotation_Greedy", index=False)
                                        auto_adjust_columns(writer, "Rotation_Greedy", rot_greedy_out)
                                        
                                        if not df_bump_greedy.empty: df_bump_greedy.to_excel(writer, sheet_name="Bump_Logistics_Greedy", index=False); auto_adjust_columns(writer, "Bump_Logistics_Greedy", df_bump_greedy)
                                    else:
                                        # MODIFIED: Drop 'Team ID', 'Round', and 'Team Members' for Round 1 Matches
                                        r1 = df_rot_greedy[df_rot_greedy['Round'] == 1].drop(columns=['Team ID', 'Round', 'Team Members'], errors='ignore')
                                        r1.to_excel(writer, sheet_name="Round_1_Matches_Greedy", index=False)
                                        auto_adjust_columns(writer, "Round_1_Matches_Greedy", r1)
                            
                            # Save the output content to variables for later
                            file_content = output.getvalue()
                            file_name_x = f"Party_{party}_Match_Analysis.xlsx"
                            
                            # Add to zip
                            zf.writestr(file_name_x, file_content)
                            
                            # Add to individual list
                            individual_party_files.append((f"Party {party}", file_name_x, file_content))

                # === FIX END: Now we are OUTSIDE the 'with' block, so the zip is finalized ===
                progress_bar.empty()
                status_text.empty()
                
                # Save to Session State for Persistence
                st.session_state.match_results = {
                    "zip_data": zip_buffer.getvalue(),
                    "individual_files": individual_party_files
                }
                
                st.success("Matching Complete!")
        
        # --- DISPLAY DOWNLOAD BUTTONS (PERSISTENT) ---
        if st.session_state.match_results:
            st.divider()
            st.subheader("Download Results")
            
            # 1. Main ZIP Download
            st.download_button(
                label="Download All Matches (ZIP)",
                data=st.session_state.match_results["zip_data"],
                file_name="recruitment_matches.zip",
                mime="application/zip"
            )
            
            # 2. Individual Downloads
            st.write("### Individual Party Sheets")
            for label, fname, data in st.session_state.match_results["individual_files"]:
                st.download_button(
                    label=f"Download {label}",
                    data=data,
                    file_name=fname,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=f"dl_btn_{fname}"
                )
