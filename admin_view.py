import streamlit as st
import gspread
import pandas as pd
import numpy as np
import networkx as nx
import re
import difflib
import io
import zipfile
import timeÂ  # Added for sleep/backoff
from io import BytesIO
from math import radians
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import haversine_distances
from gspread.exceptions import APIError # Added for error handling

# --- CONFIGURATION ---
SHEET_NAME = "OverallMatchingInformation"
ADMIN_PASSWORD = "password"

SCOPES = [
Â  Â  "https://www.googleapis.com/auth/spreadsheets",
Â  Â  "https://www.googleapis.com/auth/drive"
]

# --- CACHED RESOURCES ---
@st.cache_resource
def load_model():
Â  Â  return SentenceTransformer('all-MiniLM-L6-v2')

@st.cache_data
def load_city_database():
Â  Â  url = "https://raw.githubusercontent.com/kelvins/US-Cities-Database/main/csv/us_cities.csv"
Â  Â  try:
Â  Â  Â  Â  ref_df = pd.read_csv(url)
Â  Â  Â  Â  ref_df['MATCH_KEY'] = (ref_df['CITY'] + ", " + ref_df['STATE_CODE']).str.upper()
Â  Â  Â  Â  ref_df = ref_df.drop_duplicates(subset=['MATCH_KEY'], keep='first')
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  key: [lat, lon]
Â  Â  Â  Â  Â  Â  for key, lat, lon in zip(ref_df['MATCH_KEY'], ref_df['LATITUDE'], ref_df['LONGITUDE'])
Â  Â  Â  Â  }, list(ref_df['MATCH_KEY'])
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Failed to load city database: {e}")
Â  Â  Â  Â  return {}, []

# --- HELPERS WITH RETRY & CACHING ---

# 1. Cache the connection object so we don't re-authenticate constantly
@st.cache_resource
def get_gc():
Â  Â  if "gcp_service_account" not in st.secrets:
Â  Â  Â  Â  st.error("Missing 'gcp_service_account' in Streamlit secrets.")
Â  Â  Â  Â  st.stop()
Â  Â  creds_dict = dict(st.secrets["gcp_service_account"])
Â  Â  return gspread.service_account_from_dict(creds_dict, scopes=SCOPES)

# 2. Retry wrapper to handle 429 errors automatically
def smart_read_sheet(sheet_object):
Â  Â  """Tries to read a sheet, waits and retries if quota is hit."""
Â  Â  for n in range(5): # Try 5 times
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  return sheet_object.get_all_values()
Â  Â  Â  Â  except APIError as e:
Â  Â  Â  Â  Â  Â  if "429" in str(e):
Â  Â  Â  Â  Â  Â  Â  Â  wait_time = (2 ** n) + 1 # Exponential backoff: 2s, 3s, 5s...
Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(wait_time)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  raise e
Â  Â  return [] # Return empty if all retries fail

# 3. Cache the data fetching (TTL=600s means it refreshes every 10 mins automatically)
@st.cache_data(ttl=600)
def get_data(worksheet_name):
Â  Â  """Gets all data and standardizes headers slightly."""
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  # Open sheet but use retry logic for the actual read
Â  Â  Â  Â  sheet = gc.open(SHEET_NAME).worksheet(worksheet_name)
Â  Â  Â  Â  data = smart_read_sheet(sheet)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not data: return pd.DataFrame()
Â  Â  Â  Â  df = pd.DataFrame(data[1:], columns=data[0])
Â  Â  Â  Â  df.columns = df.columns.str.strip()
Â  Â  Â  Â  return df
Â  Â  except Exception as e:
Â  Â  Â  Â  # Avoid error spam if sheet is just missing
Â  Â  Â  Â  if worksheet_name != "PNM Information":Â 
Â  Â  Â  Â  Â  Â  passÂ 
Â  Â  Â  Â  return pd.DataFrame()

# 4. Cache the bulk loader used in "Run Matching"
@st.cache_data(ttl=600)
def load_google_sheet_data(sheet_name):
Â  Â  """
Â  Â  Loads data from Google Sheets using gspread and st.secrets.
Â  Â  """
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  sh = gc.open(sheet_name)

Â  Â  Â  Â  # Helper to get DataFrame from worksheet with retry logic
Â  Â  Â  Â  def get_df(ws_name):
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  ws = sh.worksheet(ws_name)
Â  Â  Â  Â  Â  Â  Â  Â  data = smart_read_sheet(ws)
Â  Â  Â  Â  Â  Â  Â  Â  if not data:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return pd.DataFrame()
Â  Â  Â  Â  Â  Â  Â  Â  df = pd.DataFrame(data[1:], columns=data[0])
Â  Â  Â  Â  Â  Â  Â  Â  return df
Â  Â  Â  Â  Â  Â  except gspread.WorksheetNotFound:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Worksheet '{ws_name}' not found in the spreadsheet.")
Â  Â  Â  Â  Â  Â  Â  Â  return None
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  return pd.DataFrame()

Â  Â  Â  Â  # Load specific sheets
Â  Â  Â  Â  bump_teams = get_df("Bump Teams")
Â  Â  Â  Â  party_excuses = get_df("Party Excuses")
Â  Â  Â  Â  pnm_info = get_df("PNM Information")
Â  Â  Â  Â  mem_info = get_df("Member Information")
Â  Â  Â  Â  prior_conn = get_df("Prior Connections")

Â  Â  Â  Â  return bump_teams, party_excuses, pnm_info, mem_info, prior_conn

Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"An error occurred connecting to Google Sheets: {e}")
Â  Â  Â  Â  return None, None, None, None, None

def get_setting_value(cell):
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  sheet = gc.open(SHEET_NAME).worksheet("Settings")
Â  Â  Â  Â  return sheet.acell(cell).value
Â  Â  except: return None

def update_settings(cell, value):
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  gc.open(SHEET_NAME).worksheet("Settings").update_acell(cell, value)
Â  Â  Â  Â  return True
Â  Â  except: return False

def get_max_party_count():
Â  Â  """
Â  Â  Reads the 'Party Information' sheet and calculates the max value in the 'Party' column.
Â  Â  """
Â  Â  try:
Â  Â  Â  Â  df_party = get_data("Party Information")
Â  Â  Â  Â  if df_party.empty:
Â  Â  Â  Â  Â  Â  return 4
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Find column matching "Party" (case-insensitive)
Â  Â  Â  Â  party_col = next((c for c in df_party.columns if c.lower() == 'party'), None)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if party_col:
Â  Â  Â  Â  Â  Â  # Convert to numeric, coerce errors to NaN, drop NaNs, find max
Â  Â  Â  Â  Â  Â  max_val = pd.to_numeric(df_party[party_col], errors='coerce').max()
Â  Â  Â  Â  Â  Â  if pd.notna(max_val):
Â  Â  Â  Â  Â  Â  Â  Â  return int(max_val)
Â  Â  Â  Â  return 4
Â  Â  except Exception:
Â  Â  Â  Â  return 4

def update_roster(names_list):
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  ws = gc.open(SHEET_NAME).worksheet("Settings")
Â  Â  Â  Â  ws.batch_clear(["D2:D1000"])
Â  Â  Â  Â  names_list.sort()
Â  Â  Â  Â  formatted = [[n] for n in names_list if n.strip()]
Â  Â  Â  Â  if formatted: ws.update(range_name='D2', values=formatted)
Â  Â  Â  Â  return True
Â  Â  except: return False

def get_active_roster_names():
Â  Â  # Attempt to pull from cache first via get_data if possible,Â 
Â  Â  # but since this reads 'Settings' specifically (a custom range), we try/except it.
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  sheet = gc.open(SHEET_NAME).worksheet("Settings")
Â  Â  Â  Â  # Reading a single column is usually cheap, but could fail if quota is tight
Â  Â  Â  Â  roster_data = sheet.get_values("D2:D")
Â  Â  Â  Â  names = [r[0].strip() for r in roster_data if r and r[0].strip()]
Â  Â  Â  Â  if names: return names
Â  Â  except: pass

Â  Â  # Fallback to member info (which IS cached now)
Â  Â  df_mem = get_data("Member Information")
Â  Â  if not df_mem.empty:
Â  Â  Â  Â  possible_cols = ["Full Name", "Name", "Member Name", "Member"]
Â  Â  Â  Â  found_col = None
Â  Â  Â  Â  for col in df_mem.columns:
Â  Â  Â  Â  Â  Â  if any(c.lower() in col.lower() for c in possible_cols):
Â  Â  Â  Â  Â  Â  Â  Â  found_col = col
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  if found_col: return df_mem[found_col].dropna().astype(str).str.strip().tolist()
Â  Â  return []

def update_team_ranking(team_id, new_ranking):
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  sheet = gc.open(SHEET_NAME).worksheet("Bump Teams")
Â  Â  Â  Â  cell = sheet.find(str(team_id), in_column=5)
Â  Â  Â  Â  if cell:
Â  Â  Â  Â  Â  Â  sheet.update_cell(cell.row, 6, new_ranking)
Â  Â  Â  Â  Â  Â  # Clear cache so user sees update immediately
Â  Â  Â  Â  Â  Â  get_data.clear()
Â  Â  Â  Â  Â  Â  load_google_sheet_data.clear()
Â  Â  Â  Â  Â  Â  return True
Â  Â  Â  Â  return False
Â  Â  except: return False

def batch_update_pnm_rankings(rankings_map):
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  sheet = gc.open(SHEET_NAME).worksheet("PNM Information")
Â  Â  Â  Â  all_values = smart_read_sheet(sheet) # Use smart read
Â  Â  Â  Â  if not all_values: return 0
Â  Â  Â  Â  headers = [h.lower().strip() for h in all_values[0]]
Â  Â  Â  Â Â 
Â  Â  Â  Â  try: id_idx = next(i for i, h in enumerate(headers) if 'pnm id' in h or 'id' == h)
Â  Â  Â  Â  except: id_idx = 23
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  try: rank_idx = next(i for i, h in enumerate(headers) if 'recruit rank' in h or 'average' in h)
Â  Â  Â  Â  except: rank_idx = 24

Â  Â  Â  Â  updates_count = 0
Â  Â  Â  Â  for i in range(1, len(all_values)):
Â  Â  Â  Â  Â  Â  row = all_values[i]
Â  Â  Â  Â  Â  Â  if len(row) <= id_idx: continue
Â  Â  Â  Â  Â  Â  p_id = str(row[id_idx]).strip()
Â  Â  Â  Â  Â  Â  if p_id in rankings_map:
Â  Â  Â  Â  Â  Â  Â  Â  while len(row) <= rank_idx: row.append("")
Â  Â  Â  Â  Â  Â  Â  Â  row[rank_idx] = str(rankings_map[p_id])
Â  Â  Â  Â  Â  Â  Â  Â  updates_count += 1
Â  Â  Â  Â  sheet.update(values=all_values, range_name="A1")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Clear cache
Â  Â  Â  Â  get_data.clear()
Â  Â  Â  Â  load_google_sheet_data.clear()
Â  Â  Â  Â  return updates_count
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Batch update failed: {e}")
Â  Â  Â  Â  return 0

def batch_update_team_rankings(rankings_map):
Â  Â  try:
Â  Â  Â  Â  gc = get_gc()
Â  Â  Â  Â  sheet = gc.open(SHEET_NAME).worksheet("Bump Teams")
Â  Â  Â  Â  all_values = smart_read_sheet(sheet) # Use smart read
Â  Â  Â  Â  if not all_values: return 0
Â  Â  Â  Â  headers = [h.lower().strip() for h in all_values[0]]
Â  Â  Â  Â Â 
Â  Â  Â  Â  try: id_idx = next(i for i, h in enumerate(headers) if 'team id' in h or 'id' == h)
Â  Â  Â  Â  except: id_idx = 4
Â  Â  Â  Â  try: rank_idx = next(i for i, h in enumerate(headers) if 'ranking' in h or 'rank' in h)
Â  Â  Â  Â  except: rank_idx = 5

Â  Â  Â  Â  updates_count = 0
Â  Â  Â  Â  for i in range(1, len(all_values)):
Â  Â  Â  Â  Â  Â  row = all_values[i]
Â  Â  Â  Â  Â  Â  if len(row) <= id_idx: continue
Â  Â  Â  Â  Â  Â  t_id = str(row[id_idx]).strip()
Â  Â  Â  Â  Â  Â  if t_id in rankings_map:
Â  Â  Â  Â  Â  Â  Â  Â  while len(row) <= rank_idx: row.append("")
Â  Â  Â  Â  Â  Â  Â  Â  row[rank_idx] = str(rankings_map[t_id])
Â  Â  Â  Â  Â  Â  Â  Â  updates_count += 1
Â  Â  Â  Â  sheet.update(values=all_values, range_name="A1")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Clear cache
Â  Â  Â  Â  get_data.clear()
Â  Â  Â  Â  load_google_sheet_data.clear()
Â  Â  Â  Â  return updates_count
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Batch update failed: {e}")
Â  Â  Â  Â  return 0

def auto_adjust_columns(writer, sheet_name, df):
Â  Â  worksheet = writer.sheets[sheet_name]
Â  Â  for idx, col in enumerate(df.columns):
Â  Â  Â  Â  max_len = max(df[col].astype(str).map(len).max(), len(str(col))) + 2
Â  Â  Â  Â  worksheet.set_column(idx, idx, max_len)

# --- MATCHING ALGORITHM HELPERS ---
def get_coords_offline(hometown_str, city_coords_map, all_city_keys):
Â  Â  if not isinstance(hometown_str, str): return None, None
Â  Â  key = hometown_str.strip().upper()
Â  Â  if key in city_coords_map: return city_coords_map[key]
Â  Â  matches = difflib.get_close_matches(key, all_city_keys, n=1, cutoff=0.8)
Â  Â  if matches: return city_coords_map[matches[0]]
Â  Â  return None, None

def extract_terms(row, cols):
Â  Â  text_parts = [str(row.get(c, '')).lower() for c in cols]
Â  Â  combined = ", ".join([p for p in text_parts if p != 'nan' and p.strip() != ''])
Â  Â  return [t.strip() for t in combined.split(',') if t.strip()]

def get_year_tag(year_val):
Â  Â  valid_years = ["Freshman", "Sophomore", "Junior", "Senior"]
Â  Â  if pd.isna(year_val): return None
Â  Â  raw = str(year_val).strip()
Â  Â  matches = difflib.get_close_matches(raw, valid_years, n=1, cutoff=0.6)
Â  Â  return matches[0] if matches else raw.title()

# --- MAIN PAGE ---
st.set_page_config(page_title="Recruitment Admin Dashboard", layout="wide")
st.title("Sorority Recruitment Administration Dashboard")

# Initialize Session State for Results
if "match_results" not in st.session_state:
Â  Â  st.session_state.match_results = None

if "authenticated" not in st.session_state: st.session_state.authenticated = False

if not st.session_state.authenticated:
Â  Â  pwd = st.text_input("Enter Admin Password:", type="password")
Â  Â  if pwd == ADMIN_PASSWORD:
Â  Â  Â  Â  st.session_state.authenticated = True
Â  Â  Â  Â  st.rerun()
else:
Â  Â  st.success("Logged in as Admin")

Â  Â  tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
Â  Â  Â  Â  "Settings & Roster", "Member Information", "PNM Information and Rankings",Â 
Â  Â  Â  Â  "View Bump Teams", "View Excuses", "View Prior Connections", "Run Matching"])

Â  Â  # --- TAB 1: SETTINGS ---
Â  Â  with tab1:
Â  Â  Â  Â  st.header("Event Configuration")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- MODIFIED: Pull from 'Party Information' Sheet ---
Â  Â  Â  Â  detected_party_count = get_max_party_count()
Â  Â  Â  Â  st.info(f"â„¹ï¸ **Party Count:** {detected_party_count} (Detected automatically from 'Party Information' sheet)")
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  st.header("Roster Management")
Â  Â  Â  Â  col_r1, col_r2 = st.columns(2)
Â  Â  Â  Â  with col_r1:
Â  Â  Â  Â  Â  Â  st.subheader("Option A: Sync from Sheet")
Â  Â  Â  Â  Â  Â  st.info("Pull names directly from the 'Member Information' tab.")
Â  Â  Â  Â  Â  Â  # FORCE REFRESH HERE
Â  Â  Â  Â  Â  Â  if st.button("ğŸ”„ Sync Roster from 'Member Information'"):
Â  Â  Â  Â  Â  Â  Â  Â  st.cache_data.clear() # Clear cache to ensure we get fresh data
Â  Â  Â  Â  Â  Â  Â  Â  df_source = get_data("Member Information")
Â  Â  Â  Â  Â  Â  Â  Â  if not df_source.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  possible_cols = ["Full Name", "Name", "Member Name", "Member"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  found_col = None
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for col in df_source.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if any(c.lower() in col.lower() for c in possible_cols): found_col = col; break
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if found_col:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  names = df_source[found_col].astype(str).unique().tolist()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  names = [n for n in names if n.strip()]Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if update_roster(names): st.success(f"âœ… Successfully synced {len(names)} members!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("Failed to update Settings.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("Could not find name column.")
Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("'Member Information' sheet is empty.")
Â  Â  Â  Â  with col_r2:
Â  Â  Â  Â  Â  Â  st.subheader("Option B: Upload CSV")
Â  Â  Â  Â  Â  Â  st.info("Upload a CSV file to strictly override the roster names.")
Â  Â  Â  Â  Â  Â  file = st.file_uploader("Upload Member List (CSV)", type="csv")
Â  Â  Â  Â  Â  Â  if file:
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_upload = pd.read_csv(file)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_names = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name_col = next((c for c in df_upload.columns if "name" in c.lower()), None)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if name_col: new_names = df_upload[name_col].astype(str).tolist()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: new_names = df_upload.iloc[:, 0].astype(str).tolist() if not df_upload.empty else []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Clean the names
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_names = [n for n in new_names if n.lower() != 'nan' and n.strip()]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if new_names:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"Found {len(new_names)} names in CSV.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- PREVIEW SECTION ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("Preview Extracted Names (Click to View)"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(pd.DataFrame(new_names, columns=["Names to Import"]), height=200, use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # -----------------------

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Override Roster with CSV"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if update_roster(new_names): st.success("âœ… Roster overwritten!"); st.toast("Roster Overwritten!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("Failed to update settings.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.warning("No valid names found.")
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"Error reading CSV: {e}")

Â  Â  # --- TAB 2: MEMBER INFORMATION ---
Â  Â  with tab2:
Â  Â  Â  Â  st.header("Member Information Database")
Â  Â  Â  Â  # Updated to clear cache
Â  Â  Â  Â  if st.button("ğŸ”„ Refresh Member Data"):Â 
Â  Â  Â  Â  Â  Â  st.cache_data.clear()
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  df_members = get_data("Member Information")
Â  Â  Â  Â  if not df_members.empty:
Â  Â  Â  Â  Â  Â  search_mem = st.text_input("ğŸ” Search Members:")
Â  Â  Â  Â  Â  Â  if search_mem:
Â  Â  Â  Â  Â  Â  Â  Â  mask = df_members.apply(lambda x: x.astype(str).str.contains(search_mem, case=False).any(), axis=1)
Â  Â  Â  Â  Â  Â  Â  Â  display_df = df_members[mask]
Â  Â  Â  Â  Â  Â  else: display_df = df_members
Â  Â  Â  Â  Â  Â  st.metric("Total Members", len(display_df))
Â  Â  Â  Â  Â  Â  st.dataframe(display_df, use_container_width=True)
Â  Â  Â  Â  else: st.info("No member information found.")

Â  Â  # --- TAB 3: PNM RANKINGS ---
Â  Â  with tab3:
Â  Â  Â  Â  st.header("PNM Ranking Management")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- ADDED REFRESH BUTTON FOR PNM DATA ---
Â  Â  Â  Â  if st.button("ğŸ”„ Refresh PNM & Ranking Data"):
Â  Â  Â  Â  Â  Â  st.cache_data.clear()
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  # -----------------------------------------

Â  Â  Â  Â  # Load Data
Â  Â  Â  Â  df_votes = get_data("PNM Rankings")
Â  Â  Â  Â  df_pnms_master = get_data("PNM Information") # Load master list for cross-referencing
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Identify ID column in votes
Â  Â  Â  Â  id_col_votes = None
Â  Â  Â  Â  if not df_votes.empty:
Â  Â  Â  Â  Â  Â  id_col_votes = next((c for c in df_votes.columns if 'pnm id' in c.lower() or 'id' == c.lower()), None)

Â  Â  Â  Â  if not df_votes.empty and id_col_votes:
Â  Â  Â  Â  Â  Â  # --- VALIDATION LOGIC START ---
Â  Â  Â  Â  Â  Â  st.markdown("### Ranking Validation Check")
Â  Â  Â  Â  Â  Â  st.info("Set the minimum required rankings per PNM below. The system will check if every PNM in the database meets this threshold.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  c_val1, c_val2 = st.columns([1, 2])
Â  Â  Â  Â  Â  Â  with c_val1:
Â  Â  Â  Â  Â  Â  Â  Â  min_rankings_req = st.number_input("Minimum Rankings Required", min_value=1, value=3, step=1)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Perform Check
Â  Â  Â  Â  Â  Â  validation_passed = False
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 1. Get IDs and Names from Master List
Â  Â  Â  Â  Â  Â  master_id_col = next((c for c in df_pnms_master.columns if 'pnm id' in c.lower() or 'id' == c.lower()), None)
Â  Â  Â  Â  Â  Â  master_name_col = next((c for c in df_pnms_master.columns if 'pnm name' in c.lower() or 'full name' in c.lower()), None)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not df_pnms_master.empty and master_id_col:
Â  Â  Â  Â  Â  Â  Â  Â  # Calculate counts from votes dataframe
Â  Â  Â  Â  Â  Â  Â  Â  vote_counts = df_votes[id_col_votes].astype(str).str.strip().value_counts().reset_index()
Â  Â  Â  Â  Â  Â  Â  Â  vote_counts.columns = ['PNM ID', 'Vote Count']
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Prepare Master list
Â  Â  Â  Â  Â  Â  Â  Â  validation_df = df_pnms_master[[master_id_col]].copy()
Â  Â  Â  Â  Â  Â  Â  Â  validation_df.columns = ['PNM ID']
Â  Â  Â  Â  Â  Â  Â  Â  if master_name_col:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  validation_df['Name'] = df_pnms_master[master_name_col]
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  validation_df['Name'] = "Unknown"
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  validation_df['PNM ID'] = validation_df['PNM ID'].astype(str).str.strip()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Merge Master with Vote Counts (Left Join ensures we see 0s)
Â  Â  Â  Â  Â  Â  Â  Â  validation_df = validation_df.merge(vote_counts, on='PNM ID', how='left').fillna(0)
Â  Â  Â  Â  Â  Â  Â  Â  validation_df['Vote Count'] = validation_df['Vote Count'].astype(int)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Filter for failures
Â  Â  Â  Â  Â  Â  Â  Â  failed_pnms = validation_df[validation_df['Vote Count'] < min_rankings_req]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if not failed_pnms.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{len(failed_pnms)} PNM(s) have fewer than {min_rankings_req} rankings!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(failed_pnms.sort_values(by='Vote Count'), use_container_width=True, hide_index=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("You can still sync, but averages for these PNMs will be based on incomplete data (or will be 0).")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"All {len(validation_df)} PNMs meet the minimum ranking requirement ({min_rankings_req}).")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  validation_passed = True
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Could not load PNM Master list for validation. Checking only available votes.")
Â  Â  Â  Â  Â  Â  Â  Â  # Fallback: check only counts within df_votes
Â  Â  Â  Â  Â  Â  Â  Â  counts = df_votes[id_col_votes].value_counts()
Â  Â  Â  Â  Â  Â  Â  Â  failed = counts[counts < min_rankings_req]
Â  Â  Â  Â  Â  Â  Â  Â  if not failed.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Some active IDs have fewer than {min_rankings_req} votes.")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("All currently voted-on PNMs meet requirements.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  Â  Â  # --- VALIDATION LOGIC END ---

Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  df_votes['Score'] = pd.to_numeric(df_votes['Score'], errors='coerce')

Â  Â  Â  Â  Â  Â  Â  Â  if id_col_votes and 'Score' in df_votes.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  group_cols = [id_col_votes]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name_col_votes = next((c for c in df_votes.columns if 'pnm name' in c.lower()), None)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if name_col_votes: group_cols.append(name_col_votes)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  avg_df = df_votes.groupby(group_cols)['Score'].mean().reset_index()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  avg_df.rename(columns={'Score': 'Calculated Average'}, inplace=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  avg_df = avg_df.sort_values(by='Calculated Average', ascending=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Sync Rankings")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"Ready to process {len(df_votes)} total votes across {len(avg_df)} unique PNMs.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Sync Rankings to PNM Sheet"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Syncing..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rankings_map = {str(row[id_col_votes]).strip(): round(row['Calculated Average'], 2) for idx, row in avg_df.iterrows()}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  count = batch_update_pnm_rankings(rankings_map)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"âœ… Auto-synced {count} PNM rankings!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Raw Ranking Data")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- ADDED SEARCH BAR FOR RAW RANKINGS ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rank_search = st.text_input("ğŸ” Search Raw Rankings:", key="raw_rank_search")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if rank_search:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Filter: Check if the search term exists in any column of the row
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_votes = df_votes[df_votes.astype(str).apply(lambda x: x.str.contains(rank_search, case=False).any(), axis=1)]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_votes = df_votes
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(display_votes, use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # -----------------------------------------

Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("Missing 'PNM ID' or 'Score' columns in Ranking Sheet.")
Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"Error processing rankings: {e}")
Â  Â  Â  Â  else: st.info("No votes found in 'PNM Rankings' sheet yet (or ID column missing).")
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  st.subheader("Current PNM Database")
Â  Â  Â  Â  if not df_pnms_master.empty:
Â  Â  Â  Â  Â  Â  pnm_search = st.text_input("ğŸ” Search PNM Database:")
Â  Â  Â  Â  Â  Â  display_pnm = df_pnms_master[df_pnms_master.astype(str).apply(lambda x: x.str.contains(pnm_search, case=False).any(), axis=1)] if pnm_search else df_pnms_master
Â  Â  Â  Â  Â  Â  st.dataframe(display_pnm, use_container_width=True)
Â  Â  Â  Â  else: st.info("No PNM data found.")Â Â 

Â  Â  # --- TAB 4: VIEW BUMP TEAMS ---
Â  Â  with tab4:
Â  Â  Â  Â  st.header("Bump Team Management")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- ADDED REFRESH BUTTON FOR BUMP TEAMS ---
Â  Â  Â  Â  if st.button("ğŸ”„ Refresh Bump Teams"):
Â  Â  Â  Â  Â  Â  st.cache_data.clear()
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  # -------------------------------------------

Â  Â  Â  Â  df_teams = get_data("Bump Teams")
Â  Â  Â  Â  if not df_teams.empty:
Â  Â  Â  Â  Â  Â  id_col = next((c for c in df_teams.columns if 'team id' in c.lower() or 'id' in c.lower()), df_teams.columns[4] if len(df_teams.columns)>4 else None)
Â  Â  Â  Â  Â  Â  creator_col = next((c for c in df_teams.columns if 'creator' in c.lower()), df_teams.columns[1] if len(df_teams.columns)>1 else None)
Â  Â  Â  Â  Â  Â  partners_col = next((c for c in df_teams.columns if 'partner' in c.lower()), df_teams.columns[2] if len(df_teams.columns)>2 else None)
Â  Â  Â  Â  Â  Â  rank_col = next((c for c in df_teams.columns if 'rank' in c.lower()), None)

Â  Â  Â  Â  Â  Â  if id_col and creator_col:
Â  Â  Â  Â  Â  Â  Â  Â  df_teams['display_label'] = df_teams.apply(lambda x: f"Team {x[id_col]} | {x[creator_col]}, {x.get(partners_col, '')}", axis=1)
Â  Â  Â  Â  Â  Â  Â  Â  t1, t2 = st.tabs(["Single Team Recruiter Ranking Update", "Bulk Team Recruiter Ranking Upload (CSV)"])
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with t1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  c1, c2, c3 = st.columns([3, 1, 1])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with c1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sel_label = st.selectbox("Select Team to Rank:", df_teams['display_label'].tolist())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sel_id = df_teams[df_teams['display_label'] == sel_label][id_col].values[0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with c2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cur_rank = df_teams[df_teams[id_col] == sel_id][rank_col].values[0] if rank_col else 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try: init_val = int(cur_rank)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except: init_val = 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_rank = st.number_input(f"Assign Rank:", min_value=1, value=init_val, key="team_rank_input")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with c3:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("<br>", unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Save Team Rank"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if update_team_ranking(sel_id, new_rank): st.success(f"Rank {new_rank} assigned!"); st.rerun()

Â  Â  Â  Â  Â  Â  Â  Â  with t2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info('Upload a CSV with columns: "Team ID" (or "Creator Name") and "Ranking".')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_csv = st.file_uploader("Upload Rankings CSV", type=["csv"], key="team_rank_upload")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if team_csv and st.button("Process Bulk Update"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_b = pd.read_csv(team_csv)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_b.columns = df_b.columns.str.strip().str.lower()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  b_id = next((c for c in df_b.columns if 'id' in c), None)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  b_name = next((c for c in df_b.columns if 'name' in c or 'creator' in c), None)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  b_rank = next((c for c in df_b.columns if 'rank' in c), None)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if b_rank and (b_id or b_name):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bulk_map = {}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name_map = dict(zip(df_teams[creator_col].astype(str).str.strip().str.lower(), df_teams[id_col].astype(str))) if b_name else {}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for _, r in df_b.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rv = r[b_rank]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tid = None
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if b_id and pd.notna(r[b_id]): tid = str(int(r[b_id])) if str(r[b_id]).replace('.','').isdigit() else str(r[b_id])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif b_name and pd.notna(r[b_name]): tid = name_map.get(str(r[b_name]).strip().lower())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if tid: bulk_map[tid] = rv
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if bulk_map:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cnt = batch_update_team_rankings(bulk_map)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"âœ… Updated {cnt} teams!"); st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.warning("No valid teams found.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("CSV missing required columns.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"Error: {e}")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  Â  Â  st.subheader("Current Bump Teams List")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- ADDED SEARCH BAR HERE ---
Â  Â  Â  Â  Â  Â  team_search = st.text_input("ğŸ” Search Bump Teams:", key="bump_team_search")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if team_search:
Â  Â  Â  Â  Â  Â  Â  Â  display_teams = df_teams[df_teams.astype(str).apply(lambda x: x.str.contains(team_search, case=False).any(), axis=1)]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  display_teams = df_teams
Â  Â  Â  Â  Â  Â  # -----------------------------

Â  Â  Â  Â  Â  Â  st.dataframe(display_teams.drop(columns=['display_label'], errors='ignore'), use_container_width=True)
Â  Â  Â  Â  else: st.info("No bump teams found yet.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  # --- TAB 5 & 6: EXCUSES & CONNECTIONS ---
Â  Â  with tab5:
Â  Â  Â  Â  st.header("Member Party Excuses")
Â  Â  Â  Â  if st.button("ğŸ”„ Refresh Excuses"):Â 
Â  Â  Â  Â  Â  Â  st.cache_data.clear()
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â Â 
Â  Â  Â  Â  df_ex = get_data("Party Excuses")
Â  Â  Â  Â  if not df_ex.empty:
Â  Â  Â  Â  Â  Â  # --- ADDED SEARCH BAR FOR EXCUSES ---
Â  Â  Â  Â  Â  Â  excuse_search = st.text_input("ğŸ” Search Excuses:", key="excuse_search_input")
Â  Â  Â  Â  Â  Â  if excuse_search:
Â  Â  Â  Â  Â  Â  Â  Â  display_ex = df_ex[df_ex.astype(str).apply(lambda x: x.str.contains(excuse_search, case=False).any(), axis=1)]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  display_ex = df_ex
Â  Â  Â  Â  Â  Â  # ------------------------------------
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.dataframe(display_ex, use_container_width=True)
Â  Â  Â  Â  else: st.info("No excuses found.")

Â  Â  with tab6:
Â  Â  Â  Â  st.header("Prior Member - PNM Connections")
Â  Â  Â  Â  if st.button("ğŸ”„ Refresh Connections"):Â 
Â  Â  Â  Â  Â  Â  st.cache_data.clear()
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â Â 
Â  Â  Â  Â  df_conn = get_data("Prior Connections")
Â  Â  Â  Â  if not df_conn.empty:
Â  Â  Â  Â  Â  Â  # --- ADDED SEARCH BAR FOR CONNECTIONS ---
Â  Â  Â  Â  Â  Â  conn_search = st.text_input("ğŸ” Search Prior Connections:", key="conn_search_input")
Â  Â  Â  Â  Â  Â  if conn_search:
Â  Â  Â  Â  Â  Â  Â  Â  display_conn = df_conn[df_conn.astype(str).apply(lambda x: x.str.contains(conn_search, case=False).any(), axis=1)]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  display_conn = df_conn
Â  Â  Â  Â  Â  Â  # ----------------------------------------

Â  Â  Â  Â  Â  Â  st.dataframe(display_conn, use_container_width=True)
Â  Â  Â  Â  else: st.info("No prior connections found.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  # --- TAB 7: RUN MATCHING ---

Â  Â  # --- TAB 7: RUN MATCHING ---
Â  Â  with tab7:
Â  Â  Â  Â  st.header("Run Matching Algorithm")
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.subheader("Matching Algorithm Settings")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- MODIFIED: Use the detected count from 'Party Information' ---
Â  Â  Â  Â  num_parties = get_max_party_count() # Simplified, get_max_party_count handles the try/except/default internally

Â  Â  Â  Â  st.info(f"**Total Parties:** {num_parties} (Detected from 'Party Information' sheet)")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # User input for Matches per Team and Rounds
Â  Â  Â  Â  matches_per_team = st.number_input("Matches per Bump Team (Capacity)", min_value=1, value=2)
Â  Â  Â  Â  num_rounds = st.number_input("Rounds per Party", min_value=1, value=4)
Â  Â  Â  Â  bump_order_set = st.radio("Is Bump Order Set?", ("Yes", "No"), horizontal=True)
Â  Â  Â  Â  is_bump_order_set = "y" if bump_order_set == "Yes" else "n"

Â  Â  Â  Â  st.divider()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # ADDED: User Input for PNM Party Assignments
Â  Â  Â  Â  st.subheader("Upload PNM Party Assignments")
Â  Â  Â  Â  party_assignment_file = st.file_uploader("Upload CSV containing 'PNM ID', 'PNM Name', and 'Party'", type=["csv"])
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- PREVIEW LOGIC ADDED HERE ---
Â  Â  Â  Â  if party_assignment_file:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # Read just for preview
Â  Â  Â  Â  Â  Â  Â  Â  df_preview = pd.read_csv(party_assignment_file)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # IMPORTANT: Reset file pointer to the beginning so the algorithm can read it again later
Â  Â  Â  Â  Â  Â  Â  Â  party_assignment_file.seek(0)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("Preview Uploaded Data (Click to Expand)"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"**Rows found:** {len(df_preview)}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(df_preview.head(), use_container_width=True)
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error generating preview: {e}")
Â  Â  Â  Â  # --------------------------------

Â  Â  Â  Â  run_button = st.button("Run Matching Algorithm", type="primary", use_container_width=True)

Â  Â  Â  Â  # --- MAIN LOGIC ---
Â  Â  Â  Â  if run_button:
Â  Â  Â  Â  Â  Â  if not party_assignment_file:
Â  Â  Â  Â  Â  Â  Â  Â  st.error("âŒ Please upload the PNM Party Assignments CSV to proceed.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Processing matches..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Load Data from Google Sheets
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bump_teams, party_excuses, pnm_intial_interest, member_interest, member_pnm_no_match = load_google_sheet_data(SHEET_NAME)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if any(df is None for df in [bump_teams, party_excuses, pnm_intial_interest, member_interest, member_pnm_no_match]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Clean Columns for Google Sheet Data
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for df in [bump_teams, party_excuses, pnm_intial_interest, member_interest, member_pnm_no_match]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df.columns = df.columns.str.strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Load NLP & Geo Resources
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model = load_model()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  city_coords_map, all_city_keys = load_city_database()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- STEP 1: PARTY ASSIGNMENT & CLUSTERING ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Read the uploaded assignment CSV
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  assignments_df = pd.read_csv(party_assignment_file)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  assignments_df.columns = assignments_df.columns.str.strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Validate required columns
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  required_assignment_cols = ['PNM ID', 'Party']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not all(col in assignments_df.columns for col in required_assignment_cols):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Uploaded CSV must contain columns: {required_assignment_cols}. Found: {list(assignments_df.columns)}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Prepare for merge (Ensure IDs are strings for matching)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_intial_interest['PNM ID'] = pnm_intial_interest['PNM ID'].astype(str).str.strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  assignments_df['PNM ID'] = assignments_df['PNM ID'].astype(str).str.strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Merge Google Sheet Data with Party Assignments
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Inner merge filters to only include PNMs present in the uploaded CSV
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_intial_interest = pnm_intial_interest.merge(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  assignments_df[['PNM ID', 'Party']],Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  on='PNM ID',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  how='inner')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pnm_intial_interest.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("No matches found between the uploaded PNM Assignments and the PNM Information database. Check your PNM IDs.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Ensure 'Party' is numeric for the loop
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_intial_interest['Party'] = pd.to_numeric(pnm_intial_interest['Party'], errors='coerce').fillna(0).astype(int)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error reading or processing the Party Assignments CSV: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # (Previous logic for tiling and random shuffling assignments has been removed)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Standardize PNM Columns
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_col_map = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'PNM Name': 'Full Name', 'Enter your hometown in the form City, State:': 'Hometown',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Enter your major or "Undecided":': 'Major', 'Enter your minor or leave blank:': 'Minor',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Enter your high school involvement (sports, clubs etc.), separate each activity by a comma:': 'High School Involvement',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Enter your college involvement (sports, clubs etc.), separate each activity by a comma:': 'College Involvement',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Enter your hobbies and interests, separate each activity by a comma:': 'Hobbies',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Pick your year in school:': 'Year'}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_clean = pnm_intial_interest.rename(columns=pnm_col_map)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_mem = member_interest.copy()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- CLUSTERING LOGIC ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_coords, geo_tracker = [], []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for idx, row in df_mem.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lat, lon = get_coords_offline(row.get('Hometown'), city_coords_map, all_city_keys)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if lat:Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_coords.append([radians(lat), radians(lon)])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  geo_tracker.append({'type': 'mem', 'id': row['Sorority ID'], 'hometown': row['Hometown']})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for idx, row in pnm_clean.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lat, lon = get_coords_offline(row.get('Hometown'), city_coords_map, all_city_keys)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if lat:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_coords.append([radians(lat), radians(lon)])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  geo_tracker.append({'type': 'pnm', 'id': row['PNM ID'], 'hometown': row['Hometown']})

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mem_geo_tags, pnm_geo_tags = {}, {}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if all_coords:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dist_matrix = haversine_distances(all_coords, all_coords) * 3958.8
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  geo_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=30, metric='precomputed', linkage='single')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  geo_labels = geo_clustering.fit_predict(dist_matrix)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  geo_groups = {}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, label in enumerate(geo_labels):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if label not in geo_groups: geo_groups[label] = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  geo_groups[label].append(geo_tracker[i]['hometown'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, label in enumerate(geo_labels):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  group_name = geo_groups[label][0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tracker = geo_tracker[i]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if tracker['type'] == 'mem': mem_geo_tags[tracker['id']] = group_name
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: pnm_geo_tags[tracker['id']] = group_name

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_terms_list, mem_interest_map, pnm_interest_map = [], [], []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cols_to_extract = ['Major', 'Minor', 'Hobbies', 'College Involvement', 'High School Involvement']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for idx, row in df_mem.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  terms = extract_terms(row, cols_to_extract)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for term in terms: all_terms_list.append(term); mem_interest_map.append({'id': row['Sorority ID'], 'term': term})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for idx, row in pnm_clean.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  terms = extract_terms(row, cols_to_extract)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for term in terms: all_terms_list.append(term); pnm_interest_map.append({'id': row['PNM ID'], 'term': term})

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  term_to_group = {}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if all_terms_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  unique_terms = list(set(all_terms_list))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  embeddings = model.encode(unique_terms)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sem_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5, metric='cosine', linkage='average')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sem_labels = sem_clustering.fit_predict(embeddings)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temp_map = {}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for term, label in zip(unique_terms, sem_labels):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if label not in temp_map: temp_map[label] = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temp_map[label].append(term)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for label, terms in temp_map.items():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  attr_name = min(terms, key=len)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for term in terms: term_to_group[term] = attr_name

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  def finalize_attributes(df, id_col, geo_tags, int_map):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_attrs = {row[id_col]: set() for _, row in df.iterrows()}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for idx, row in df.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pid = row[id_col]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  yt = get_year_tag(row.get('Year'))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if yt: final_attrs[pid].add(yt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pid in geo_tags: final_attrs[pid].add(geo_tags[pid])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for entry in int_map:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pid = entry['id']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if entry['term'] in term_to_group: final_attrs[pid].add(term_to_group[entry['term']])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return df[id_col].map(lambda x: ", ".join(final_attrs.get(x, set())))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  member_interest['attributes_for_matching'] = finalize_attributes(df_mem, 'Sorority ID', mem_geo_tags, mem_interest_map)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_intial_interest['attributes_for_matching'] = finalize_attributes(pnm_clean, 'PNM ID', pnm_geo_tags, pnm_interest_map)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- STEP 2: CALCULATE GLOBAL RANKING STATS (PNM & RECRUITER) ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 1. PNM Stats
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_ranks = pd.to_numeric(pnm_intial_interest['Average Recruit Rank'], errors='coerce')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_obs = all_ranks.min()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pd.isna(min_obs): min_obs = 1.0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_ranks = all_ranks.fillna(min_obs)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_max = all_ranks.max()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_min = all_ranks.min()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if global_max == global_min: global_max += 1.0Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 2. Recruiter (Team) Stats
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_team_ranks = pd.to_numeric(bump_teams['Ranking'], errors='coerce')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_t_obs = all_team_ranks.min()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pd.isna(min_t_obs): min_t_obs = 1.0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_team_ranks = all_team_ranks.fillna(4.0) # Default if missing
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_global_max = all_team_ranks.max()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_global_min = all_team_ranks.min()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if t_global_max == t_global_min: t_global_max += 1.0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error calculating global ranking stats: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_max, global_min = 5.0, 1.0Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_global_max, t_global_min = 4.0, 1.0

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- STEP 3: CORE MATCHING LOGIC ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  zip_buffer = BytesIO()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Pre-process Data for Loop
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  party_excuses["Choose the party/parties you are unable to attend:"] = party_excuses["Choose the party/parties you are unable to attend:"].apply(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lambda x: [int(i) for i in re.findall(r'\d+', str(x))] if pd.notnull(x) else []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  party_excuses = party_excuses.explode("Choose the party/parties you are unable to attend:")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  member_pnm_no_match["PNM Name"] = member_pnm_no_match["PNM Name"].str.split(r',\s*', regex=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  member_pnm_no_match = member_pnm_no_match.explode("PNM Name")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  no_match_pairs = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (row["Member Name"], row["PNM Name"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for row in member_pnm_no_match.to_dict('records')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  member_attr_cache = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  row['Sorority ID']: set(str(row.get('attributes_for_matching', '')).split(', '))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if row.get('attributes_for_matching') else set()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for row in member_interest.to_dict('records')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name_to_id_map = member_interest.set_index('Full Name')['Sorority ID'].to_dict()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_member_traits = member_interest['attributes_for_matching'].str.split(', ').explode()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  trait_freq = all_member_traits.value_counts()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  trait_weights = (len(member_interest) / trait_freq).to_dict()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Conversion helper for strings from GSheet
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  def to_float(val, default=1.0):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try: return float(val)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except: return default
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  def to_int(val, default=4):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try: return int(val)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except: return default

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # List to store individual file data for later download buttons
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  individual_party_files = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for party in range(1, int(num_parties) + 1):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnms_df = pnm_intial_interest[pnm_intial_interest['Party'] == party].copy()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pnms_df.empty: continue

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_list = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_records = pnms_df.to_dict('records')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, row in enumerate(pnm_records):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_attrs = set(str(row['attributes_for_matching']).split(', '))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_rank_val = to_float(row.get("Average Recruit Rank", 1.0))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- AGNOSTIC BONUS CALCULATION ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  safe_rank = max(global_min, min(p_rank_val, global_max))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  relative_strength = (safe_rank - global_min) / (global_max - global_min)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  RANKING_WEIGHT = 3.0Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_bonus = relative_strength * RANKING_WEIGHT
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ----------------------------------

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_list.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'idx': i,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'id': row['PNM ID'],Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'name': row.get('PNM Name', row.get('Full Name')),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'attrs': p_attrs,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'rank': p_rank_val,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'bonus': pnm_bonus,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'node_id': f"p_{i}"})

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  party_excused_names = set(party_excuses[party_excuses["Choose the party/parties you are unable to attend:"] == party]["Member Name"])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_list = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  broken_teams_list = []

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for raw_idx, row in enumerate(bump_teams.to_dict('records')):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  submitter = row["Creator Name"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  partners_str = str(row.get("Bump Partners", ""))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if partners_str.lower() == 'nan': partners = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: partners = [p.strip() for p in re.split(r'[,;]\s*', partners_str) if p.strip()]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_members = [submitter] + partners
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  missing_members = [m for m in current_members if m in party_excused_names]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if missing_members:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  broken_teams_list.append({'members': current_members, 'missing': missing_members})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_rank_val = to_float(row.get("Ranking", t_global_max)) # Default to worst rank if missing
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- RECRUITER AGNOSTIC BONUS ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  safe_t_rank = max(t_global_min, min(t_rank_val, t_global_max))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_rel_strength = (t_global_max - safe_t_rank) / (t_global_max - t_global_min)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  TEAM_WEIGHT = 1.5Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_bonus = team_rel_strength * TEAM_WEIGHT
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --------------------------------

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_list.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  't_idx': len(team_list), 'members': current_members, 'team_size': len(current_members),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'member_ids': [name_to_id_map.get(m) for m in current_members],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'joined_names': ", ".join(current_members), 'bonus': t_bonus,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'node_id': f"t_{len(team_list)}", 'row_data': row
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- Capacity Checks ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_capacity = len(team_list) * matches_per_team
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # NEW VALIDATION
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(pnm_list) > total_capacity:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  warning_msg = (f"**Party {party} Warning**: Not enough capacity!\n {len(pnm_list)} PNMs vs {total_capacity} Slots ({len(team_list)} Teams Ã— {matches_per_team}). \n Unmatched PNMs will appear in the results.\n\n")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Check for excused teams
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if broken_teams_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  warning_msg += f"- **Excused Teams:** {len(broken_teams_list)} team(s) removed due to excuses:\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for b_team in broken_teams_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_display = ", ".join(b_team['members'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  excused_person = ", ".join(b_team['missing'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  warning_msg += f"Â  - Team {team_display} removed due to (Excused: **{excused_person}**)\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  warning_msg += "No teams were removed due to excuses for this party.\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  warning_msg += "\n" # Spacing
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Check for relevant No-Match restrictions
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  active_team_members = set()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for t in team_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  active_team_members.update(t['members'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_names_in_party = {p['name'] for p in pnm_list}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  relevant_conflicts = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for (m_name, p_name) in no_match_pairs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if p_name in pnm_names_in_party and m_name in active_team_members:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  relevant_conflicts.append(f"Sorority Member {m_name} and PNM {p_name}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if relevant_conflicts:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  warning_msg += f"- **Active No-Match Constraints:** {len(relevant_conflicts)} pair(s) found:\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for conf in relevant_conflicts:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  warning_msg += f"Â  - {conf}\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  warning_msg += "No conflicts found between present PNMs and Members.\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(warning_msg)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  potential_pairs = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p_data in pnm_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for t_data in team_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if any((m, p_data['name']) in no_match_pairs for m in t_data['members']): continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score = 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reasons_list = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for m_id, m_name in zip(t_data['member_ids'], t_data['members']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if m_id is None: continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  m_attrs = member_attr_cache.get(m_id, set())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shared = p_data['attrs'].intersection(m_attrs)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if shared:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += sum(trait_weights.get(t, 1.0) for t in shared)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reasons_list.append(f"{p_data['name']} has {', '.join(shared)} with {m_name}.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_score = score + t_data['bonus'] + p_data['bonus']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_cost = 1 / (1 + total_score)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  potential_pairs.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'p_id': p_data['id'], 'p_name': p_data['name'], 'p_attrs': p_data['attrs'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  't_idx': t_data['t_idx'], 'team_size': t_data['team_size'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'p_node': p_data['node_id'], 't_node': t_data['node_id'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'cost': final_cost, 'pnm_rank': p_data['rank'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'team_members': t_data['joined_names'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'reasons': " ".join(reasons_list) if reasons_list else "No specific match"})

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  potential_pairs.sort(key=lambda x: (x['cost'], -x['pnm_rank']))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  matchable_pnm_ids = {p['p_id'] for p in potential_pairs}

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- PHASE A: GLOBAL MATCHING ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_flow_results = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  assignments_map_flow = {t['t_idx']: [] for t in team_list}

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G = nx.DiGraph()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source, sink, no_match_node = 'source', 'sink', 'dummy_nomatch'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_flow = len(pnm_list)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G.add_node(source, demand=-total_flow)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G.add_node(sink, demand=total_flow)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G.add_node(no_match_node)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p in pnm_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G.add_edge(source, p['node_id'], capacity=1, weight=0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G.add_edge(p['node_id'], no_match_node, capacity=1, weight=1000000)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for t in team_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G.add_edge(t['node_id'], sink, capacity=matches_per_team, weight=0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G.add_edge(no_match_node, sink, capacity=total_flow, weight=0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for pair in potential_pairs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  G.add_edge(pair['p_node'], pair['t_node'], capacity=1, weight=int(pair['cost'] * 10000))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  flow_dict = nx.min_cost_flow(G)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pair_lookup = {(p['p_node'], p['t_node']): p for p in potential_pairs}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pnm_ids_with_potential = {p['p_id'] for p in potential_pairs}

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p_data in pnm_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_node = p_data['node_id']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if p_node in flow_dict:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for t_node, flow in flow_dict[p_node].items():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if flow > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if t_node == no_match_node:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reason = "Conflict List" if p_data['id'] not in pnm_ids_with_potential else "Capacity Reached"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_flow_results.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'PNM ID': p_data['id'], 'PNM Name': p_data['name'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Bump Team Members': "NO MATCH", 'Match Cost': None, 'Reason': reason
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  match_info = pair_lookup.get((p_node, t_node))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if match_info:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_flow_results.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'PNM ID': p_data['id'], 'PNM Name': p_data['name'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Bump Team Members': match_info['team_members'], 'Match Cost': round(match_info['cost'], 4),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Reason': match_info['reasons']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  assignments_map_flow[match_info['t_idx']].append(match_info)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except nx.NetworkXUnfeasible:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Global Flow Unfeasible for Party {party}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- A2: GLOBAL GREEDY ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_greedy_results = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  assignments_map_greedy = {t['t_idx']: [] for t in team_list}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  matched_pnm_ids = set()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_counts = {t['t_idx']: 0 for t in team_list}

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for pair in potential_pairs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pair['p_id'] not in matched_pnm_ids:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if team_counts[pair['t_idx']] < matches_per_team:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  matched_pnm_ids.add(pair['p_id'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_counts[pair['t_idx']] += 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_greedy_results.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'PNM ID': pair['p_id'], 'PNM Name': pair['p_name'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Bump Team Members': pair['team_members'], 'Match Cost': round(pair['cost'], 4),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Reason': pair['reasons']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  assignments_map_greedy[pair['t_idx']].append(pair)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p_data in pnm_list:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if p_data['id'] not in matched_pnm_ids:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  was_blocked = not any(p['p_id'] == p_data['id'] for p in potential_pairs)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reason = "Conflict List" if was_blocked else "Capacity Reached (Greedy)"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_greedy_results.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'PNM ID': p_data['id'], 'PNM Name': p_data['name'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Bump Team Members': "NO MATCH", 'Match Cost': None, 'Reason': reason
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- PHASE B: INTERNAL ROTATIONS ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  def run_internal_rotation(assignment_map, method='flow'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rotation_output = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  actual_rounds = 1 if is_bump_order_set == 'y' else num_rounds

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for t_idx, assigned_pnms in assignment_map.items():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not assigned_pnms: continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_data = next((t for t in team_list if t['t_idx'] == t_idx), None)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not team_data: continue

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raw_rgl = team_data['row_data'].get('RGL', '')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  team_rgl_name = "" if pd.isna(raw_rgl) or str(raw_rgl).lower() == 'nan' else str(raw_rgl).strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  valid_members = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for m_id, m_name in zip(team_data['member_ids'], team_data['members']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if m_id: valid_members.append({'id': m_id, 'name': m_name})

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  history = set()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for round_num in range(1, actual_rounds + 1):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if round_num == 1 and team_rgl_name:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  active_members = [m for m in valid_members if m['name'].strip() != team_rgl_name]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  active_members = valid_members

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  must_allow_repeats = round_num > len(active_members)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if method == 'flow':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sub_G = nx.DiGraph()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sub_s, sub_t = 's', 't'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  req = len(assigned_pnms)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sub_G.add_node(sub_s, demand=-req)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sub_G.add_node(sub_t, demand=req)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p in assigned_pnms: sub_G.add_edge(sub_s, f"p_{p['p_id']}", capacity=1, weight=0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for m in active_members: sub_G.add_edge(f"m_{m['id']}", sub_t, capacity=1, weight=0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p in assigned_pnms:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for m in active_members:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Ensure string comparison for history check
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_repeat = (str(p['p_id']), str(m['id'])) in history
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if is_repeat and not must_allow_repeats: continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  m_attrs = member_attr_cache.get(m['id'], set())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shared = p['p_attrs'].intersection(m_attrs)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score = sum(trait_weights.get(t, 1.0) for t in shared)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  base_cost = int((1/(1+score))*10000)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_cost = base_cost + 50000 if is_repeat else base_cost
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reason = ", ".join(shared) if shared else "Rotation"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if is_repeat: reason += " (Repeat)"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sub_G.add_edge(f"p_{p['p_id']}", f"m_{m['id']}", capacity=1, weight=final_cost, reason=reason)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sub_flow = nx.min_cost_flow(sub_G)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p in assigned_pnms:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_node = f"p_{p['p_id']}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if p_node in sub_flow:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for tgt, flow in sub_flow[p_node].items():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if flow > 0 and tgt != sub_t:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Fix: Keep ID as string to match Google Sheets data format
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raw_id = tgt.replace("m_", "")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  m_name = next((m['name'] for m in valid_members if str(m['id']) == raw_id), "Unknown")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  edge_d = sub_G.get_edge_data(p_node, tgt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  calc_cost = (edge_d.get('weight', 10000) - (50000 if edge_d.get('weight',0) > 40000 else 0)) / 10000.0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rotation_output.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Round': round_num, 'Team ID': t_idx, 'Team Members': team_data['joined_names'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'PNM ID': p['p_id'], 'PNM Name': p['p_name'], 'Matched Member': m_name,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Match Cost': round(calc_cost, 4), 'Reason': f"Common: {edge_d.get('reason')}"})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add to history as strings
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  history.add((str(p['p_id']), str(raw_id)))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except nx.NetworkXUnfeasible:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rotation_output.append({'Round': round_num, 'Team ID': t_idx, 'PNM Name': "FLOW FAIL", 'Reason': "Unfeasible"})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif method == 'greedy':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  candidates = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p in assigned_pnms:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for m in active_members:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_repeat = (str(p['p_id']), str(m['id'])) in history
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if is_repeat and not must_allow_repeats: continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  m_attrs = member_attr_cache.get(m['id'], set())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shared = p['p_attrs'].intersection(m_attrs)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score = sum(trait_weights.get(t, 1.0) for t in shared)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_score = score - 1000 if is_repeat else score
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reason = ", ".join(shared) if shared else "Rotation"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if is_repeat: reason += " (Repeat)"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  candidates.append((final_score, p, m, reason, is_repeat))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  candidates.sort(key=lambda x: x[0], reverse=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round_pnm_done, round_mem_done = set(), set()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for sc, p, m, rs, is_rep in candidates:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if p['p_id'] not in round_pnm_done and m['id'] not in round_mem_done:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  real_score = sc + 1000 if is_rep else sc
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rotation_output.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Round': round_num, 'Team ID': t_idx, 'Team Members': team_data['joined_names'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'PNM ID': p['p_id'], 'PNM Name': p['p_name'], 'Matched Member': m['name'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Match Cost': round(1.0/(1.0+real_score), 4), 'Reason': f"Common: {rs}" if real_score > 0 else "Greedy Fill"})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round_pnm_done.add(p['p_id']); round_mem_done.add(m['id'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  history.add((str(p['p_id']), str(m['id'])))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return rotation_output

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  internal_flow_results = run_internal_rotation(assignments_map_flow, method='flow')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  internal_greedy_results = run_internal_rotation(assignments_map_greedy, method='greedy')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- PHASE C: BUMP INSTRUCTIONS ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  def generate_bump_instructions(rotation_data):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not rotation_data: return []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df = pd.DataFrame(rotation_data)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if df.empty or 'Matched Member' not in df.columns: return []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df = df.sort_values(by=['Team ID', 'PNM ID', 'Round'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df['Person_To_Bump'] = df.groupby(['Team ID', 'PNM ID'])['Matched Member'].shift(1)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  instructions = df[df['Person_To_Bump'].notna()].copy()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  instructions['At End Of Round'] = instructions['Round'] - 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  output = instructions[['Matched Member', 'At End Of Round', 'Person_To_Bump', 'PNM Name']].rename(columns={
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Matched Member': 'Member (You)', 'Person_To_Bump': 'Go Bump This Person', 'PNM Name': 'Who is with PNM'})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return output.sort_values(by=['Member (You)', 'At End Of Round']).to_dict('records')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bump_instruct_flow = generate_bump_instructions(internal_flow_results)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bump_instruct_greedy = generate_bump_instructions(internal_greedy_results)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- EXPORT TO EXCEL ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if global_flow_results:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  output = BytesIO()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_glob_flow = pd.DataFrame(global_flow_results)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_glob_greedy = pd.DataFrame(global_greedy_results)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_rot_flow = pd.DataFrame(internal_flow_results)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_rot_greedy = pd.DataFrame(internal_greedy_results)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_bump_flow = pd.DataFrame(bump_instruct_flow)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_bump_greedy = pd.DataFrame(bump_instruct_greedy)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- SUMMARY CALCULATION ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  flow_costs = df_glob_flow['Match Cost'].dropna()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  greedy_costs = df_glob_greedy['Match Cost'].dropna()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  summary_data = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Metric': ['Total Cost', 'Average Cost', 'Min Cost', 'Max Cost', 'Std Dev'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Global Network Flow': [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(flow_costs.sum(), 4),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(flow_costs.mean(), 4) if not flow_costs.empty else 0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(flow_costs.min(), 4) if not flow_costs.empty else 0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(flow_costs.max(), 4) if not flow_costs.empty else 0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(flow_costs.std(), 4) if len(flow_costs) > 1 else 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Global Greedy': [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(greedy_costs.sum(), 4),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(greedy_costs.mean(), 4) if not greedy_costs.empty else 0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(greedy_costs.min(), 4) if not greedy_costs.empty else 0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(greedy_costs.max(), 4) if not greedy_costs.empty else 0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  round(greedy_costs.std(), 4) if len(greedy_costs) > 1 else 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  summary_df = pd.DataFrame(summary_data)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  summary_df.to_excel(writer, sheet_name="Summary_Comparison", index=False); auto_adjust_columns(writer, "Summary_Comparison", summary_df)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_glob_flow.to_excel(writer, sheet_name="Global_Matches_Flow", index=False); auto_adjust_columns(writer, "Global_Matches_Flow", df_glob_flow)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_glob_greedy.to_excel(writer, sheet_name="Global_Matches_Greedy", index=False); auto_adjust_columns(writer, "Global_Matches_Greedy", df_glob_greedy)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not df_rot_flow.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if is_bump_order_set == "n":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # MODIFIED: Drop 'Team ID' and 'Team Members' for Rotation Flow export
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rot_flow_out = df_rot_flow.drop(columns=['Team ID', 'Team Members'], errors='ignore')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rot_flow_out.to_excel(writer, sheet_name="Rotation_Flow", index=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  auto_adjust_columns(writer, "Rotation_Flow", rot_flow_out)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not df_bump_flow.empty: df_bump_flow.to_excel(writer, sheet_name="Bump_Logistics_Flow", index=False); auto_adjust_columns(writer, "Bump_Logistics_Flow", df_bump_flow)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # MODIFIED: Drop 'Team ID', 'Round', and 'Team Members' for Round 1 Matches
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r1 = df_rot_flow[df_rot_flow['Round'] == 1].drop(columns=['Team ID', 'Round', 'Team Members'], errors='ignore')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r1.to_excel(writer, sheet_name="Round_1_Matches_Flow", index=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  auto_adjust_columns(writer, "Round_1_Matches_Flow", r1)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not df_rot_greedy.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if is_bump_order_set == "n":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # MODIFIED: Drop 'Team ID' and 'Team Members' for Rotation Greedy export
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rot_greedy_out = df_rot_greedy.drop(columns=['Team ID', 'Team Members'], errors='ignore')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rot_greedy_out.to_excel(writer, sheet_name="Rotation_Greedy", index=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  auto_adjust_columns(writer, "Rotation_Greedy", rot_greedy_out)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not df_bump_greedy.empty: df_bump_greedy.to_excel(writer, sheet_name="Bump_Logistics_Greedy", index=False); auto_adjust_columns(writer, "Bump_Logistics_Greedy", df_bump_greedy)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # MODIFIED: Drop 'Team ID', 'Round', and 'Team Members' for Round 1 Matches
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r1 = df_rot_greedy[df_rot_greedy['Round'] == 1].drop(columns=['Team ID', 'Round', 'Team Members'], errors='ignore')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r1.to_excel(writer, sheet_name="Round_1_Matches_Greedy", index=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  auto_adjust_columns(writer, "Round_1_Matches_Greedy", r1)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Save the output content to variables for later
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_content = output.getvalue()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name_x = f"Party_{party}_Match_Analysis.xlsx"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add to zip
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  zf.writestr(file_name_x, file_content)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add to individual list
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  individual_party_files.append((f"Party {party}", file_name_x, file_content))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Save to Session State for Persistence
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.match_results = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "zip_data": zip_buffer.getvalue(),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "individual_files": individual_party_files
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("Matching Complete!")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- DISPLAY DOWNLOAD BUTTONS (PERSISTENT) ---
Â  Â  Â  Â  if st.session_state.match_results:
Â  Â  Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  Â  Â  st.subheader("Download Results")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 1. Main ZIP Download
Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  label="Download All Matches (ZIP)",
Â  Â  Â  Â  Â  Â  Â  Â  data=st.session_state.match_results["zip_data"],
Â  Â  Â  Â  Â  Â  Â  Â  file_name="recruitment_matches.zip",
Â  Â  Â  Â  Â  Â  Â  Â  mime="application/zip")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 2. Individual Downloads
Â  Â  Â  Â  Â  Â  st.write("### Individual Party Sheets")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  files = st.session_state.match_results["individual_files"]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Define how many buttons you want per row (e.g., 4)
Â  Â  Â  Â  Â  Â  cols_per_row = 4
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Loop through the files in chunks to create rows
Â  Â  Â  Â  Â  Â  for i in range(0, len(files), cols_per_row):
Â  Â  Â  Â  Â  Â  Â  Â  # Get the batch of files for this row
Â  Â  Â  Â  Â  Â  Â  Â  row_files = files[i : i + cols_per_row]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Create the columns for this row
Â  Â  Â  Â  Â  Â  Â  Â  cols = st.columns(cols_per_row)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Place buttons inside the columns
Â  Â  Â  Â  Â  Â  Â  Â  for idx, (label, fname, data) in enumerate(row_files):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with cols[idx]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label=f"Download {label}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=data,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=fname,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key=f"dl_btn_{fname}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=TrueÂ  # This makes the button stretch to fill the column
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
